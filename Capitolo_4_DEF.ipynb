{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8848b5db",
   "metadata": {},
   "source": [
    "# **Capitolo 4 — Pipeline completa**\n",
    "\n",
    "Questo capitolo costruisce una pipeline riproducibile per:\n",
    "1) **Parsing e labeling** del dataset (Step 0–1)\n",
    "2) **Pulizia/normalizzazione** + indici sintetici (Step 2–2B)\n",
    "3) **KPI descrittivi** e tabelle per fascia/coorte (Step 3)\n",
    "4) **Derivate robuste, grafici, test statistici** (Step 4A–4C)\n",
    "5) **Modellistica esplicativa** (MNLogit) e diagnostica (VIF) (Step 4D–4E)\n",
    "6) **Modellistica predittiva** + importanze/permutation (Step 4F)\n",
    "7) **PCA** e MNLogit su componenti ortogonali (Step 4G)\n",
    "8) **Logistic multinomiale regolarizzata** con CV (Step 4H)\n",
    "9) **Calibrazione probabilità** (Brier) (Step 4I)\n",
    "10) **Partial Dependence (PDP)** per interpretabilità di HGB (Step 4J)\n",
    "\n",
    "Ogni step salva i propri **output intermedi** in `C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito`\n",
    "e prepara i **passi successivi**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28653a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6b67c9",
   "metadata": {},
   "source": [
    "## **STEP 0 — Fix parsing**\n",
    "**Logica:** rileva automaticamente il separatore del CSV, decide se promuovere la prima riga a header\n",
    "tramite un'euristica basata su parole-chiave del questionario e struttura testuale.\n",
    "\n",
    "**Output:** `_dataset_semicolon.csv` (versione “riparata” del file grezzo).\n",
    "\n",
    "**Next:** lo Step 1 userà questo file per individuare il “vero” header e mappare le colonne a nomi canonici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f224b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separator candidate counts: {',': 78, ';': 218, '\\t': 0, '|': 0} -> chosen: ;\n",
      "Initial shape (no header): (1066, 25)\n",
      "Promote first row as header? True\n",
      "Final shape: (1065, 25)\n",
      "Column sample: ['Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9']\n",
      "Saved parsed: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\_dataset_semicolon.csv\n"
     ]
    }
   ],
   "source": [
    "# STEP 0 — Fix parsing: rileva separatore e promuovi header se necessario\n",
    "import re, io, csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Percorsi (aggiorna la cartella)\n",
    "PATH_IN  = \"dataset.csv\"   # se stai eseguendo nella stessa cartella\n",
    "OUT_DIR  = Path(r\"C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PATH_OUT = OUT_DIR / \"_dataset_semicolon.csv\"\n",
    "\n",
    "# === 1) Tenta di rilevare il separatore\n",
    "delims = [\",\",\";\",\"\\t\",\"|\"]\n",
    "sample = \"\"\n",
    "with open(PATH_IN, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            sample += next(f)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "# Conta i separatori più probabili nella prima manciata di righe\n",
    "counts = {d: sample.count(d) for d in delims}\n",
    "best_sep = max(counts, key=counts.get)\n",
    "if counts[best_sep] == 0:\n",
    "    # fallback tipico per Google Forms in italiano: ';'\n",
    "    best_sep = \";\"\n",
    "\n",
    "print(\"Separator candidate counts:\", counts, \"-> chosen:\", best_sep)\n",
    "\n",
    "# === 2) Leggi una prima volta senza header (per capire se la prima riga è header testuale)\n",
    "df_try = pd.read_csv(PATH_IN, sep=best_sep, engine=\"python\", header=None)\n",
    "print(\"Initial shape (no header):\", df_try.shape)\n",
    "\n",
    "# euristica: se la prima riga contiene tante stringhe lunghe e parole chiave del tuo questionario, è un header\n",
    "KEYWORDS = [\n",
    "    \"fascia d'età\",\"occupazione\",\"titolo di studio\",\"fonte di reddito\",\n",
    "    \"principale\",\"fornitore\",\"conti correnti\",\"IBAN\",\n",
    "    \"banche TRADIZIONALI\",\"Fiducia\",\"Sicurezza\",\"Costi\",\"Commissioni\",\n",
    "    \"Innovazione\",\"Tecnologia\",\"Facilità d'uso\",\"Servizio Clienti\",\n",
    "    \"FINTECH\",\"servizi/app Fintech\",\"Importanti\",\"importanza\",\n",
    "    \"Assenza di costi fissi\",\"reputazione\",\"apertura\",\n",
    "    \"filiale fisica\",\"app mobile\",\"servizi innovativi\",\"Promozioni\",\"cashback\",\n",
    "    \"prossimi 2-3 anni\",\"gestire la maggior parte\"\n",
    "]\n",
    "kw_regex = re.compile(\"|\".join([re.escape(k) for k in KEYWORDS]), re.IGNORECASE)\n",
    "\n",
    "def looks_like_header(row_vals):\n",
    "    vals = [str(x or \"\").strip() for x in row_vals]\n",
    "    nonnum = sum(1 for v in vals if not re.fullmatch(r\"-?\\d+(?:[.,]\\d+)?\", v))\n",
    "    hits = sum(1 for v in vals if kw_regex.search(v))\n",
    "    len_ok = sum(1 for v in vals if 1 <= len(v) <= 120)\n",
    "    score = hits*2 + nonnum + len_ok*0.1\n",
    "    return score >= max(6, df_try.shape[1]*0.5)\n",
    "\n",
    "use_header = looks_like_header(df_try.iloc[0].tolist())\n",
    "print(\"Promote first row as header?\", use_header)\n",
    "\n",
    "if use_header:\n",
    "    cols = [str(x).strip() for x in df_try.iloc[0]]\n",
    "    df = df_try.iloc[1:].reset_index(drop=True)\n",
    "    df.columns = cols\n",
    "else:\n",
    "    df = pd.read_csv(PATH_IN, sep=best_sep, engine=\"python\", header=0)\n",
    "    if df.shape[1] == 1 and best_sep != \";\":\n",
    "        df = pd.read_csv(PATH_IN, sep=\";\", engine=\"python\", header=0)\n",
    "    if df.shape[1] == 1:\n",
    "        df = pd.read_csv(PATH_IN, sep=best_sep, engine=\"python\", header=None)\n",
    "        cols = [f\"col_{i+1}\" for i in range(df.shape[1])]\n",
    "        df.columns = cols\n",
    "\n",
    "print(\"Final shape:\", df.shape)\n",
    "print(\"Column sample:\", list(df.columns)[:8])\n",
    "\n",
    "df.to_csv(PATH_OUT, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved parsed:\", PATH_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c34373",
   "metadata": {},
   "source": [
    "## **STEP 1 — Setup & Profilo dati**\n",
    "**Logica:** carica `_dataset_semicolon.csv`, cerca la riga con i **veri header** (basata su keyword e non‐numericità),\n",
    "normalizza stringhe e **mappa** i nomi originali su **etichette canoniche** per l’analisi.\n",
    "\n",
    "**Output:** `_dataset_parsed_labeled.csv` con intestazioni pulite e campi rinominati.\n",
    "\n",
    "**Next:** lo Step 2 userà queste colonne per normalizzare le scale Likert, derivare indici sintetici e coorti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70fa9671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\_dataset_semicolon.csv shape: (1065, 25)\n",
      "Header candidate row: 0 score: 71 scores: [(0, 71), (4, 13), (2, 12), (3, 12), (1, 10)]\n",
      "Promossa riga 0 a header. Colonne: 25\n",
      "\n",
      "Mappatura colonne (canonico <= originale):\n",
      "-               age_band <= A quale fascia d'età appartieni?\n",
      "-             occupation <= Qual è la tua attuale occupazione principale?\n",
      "-              education <= Qual è il tuo più alto titolo di studio conseguito?\n",
      "-          income_source <= Qual è la tua principale fonte di reddito/sostentamento?\n",
      "-       primary_provider <= Quale tipologia di istituto consideri come il tuo principale \"fornitore\" di servizi finanziari (dove accrediti lo stipendio o gestisci la maggior parte delle tue entrate/uscite)?\n",
      "-         accounts_count <= Complessivamente quanti conti correnti o carte con IBAN (tradizionali e/o digitali) possiedi?\n",
      "-             trad_trust <= Esprimi la tua valutazione sulle banche TRADIZIONALI per ciascuno dei seguenti fattori (usa una scala da 1 a 5 dove 1 è Molto Negativa e 5 è Molto Positiva). [Fiducia e Sicurezza percepita]\n",
      "-              trad_fees <= Esprimi la tua valutazione sulle banche TRADIZIONALI per ciascuno dei seguenti fattori (usa una scala da 1 a 5 dove 1 è Molto Negativa e 5 è Molto Positiva). [Costi e Commissioni]\n",
      "-             trad_innov <= Esprimi la tua valutazione sulle banche TRADIZIONALI per ciascuno dei seguenti fattori (usa una scala da 1 a 5 dove 1 è Molto Negativa e 5 è Molto Positiva). [Innovazione e Tecnologia]\n",
      "-                trad_ux <= Esprimi la tua valutazione sulle banche TRADIZIONALI per ciascuno dei seguenti fattori (usa una scala da 1 a 5 dove 1 è Molto Negativa e 5 è Molto Positiva). [Facilità d'uso (App/Sito Web)]\n",
      "-           trad_service <= Esprimi la tua valutazione sulle banche TRADIZIONALI per ciascuno dei seguenti fattori (usa una scala da 1 a 5 dove 1 è Molto Negativa e 5 è Molto Positiva). [Qualità del Servizio Clienti (in filiale o telefonico)]\n",
      "- fintech_services_multi <= Quali dei seguenti tipi di servizi/app Fintech utilizzi anche occasionalmente? (Puoi selezionare più di una risposta)\n",
      "-              fin_trust <= Esprimi la tua valutazione sulle app/servizi FINTECH per ciascuno dei seguenti fattori (usa una scala da 1 a 5 dove 1 è Molto Negativa e 5 è Molto Positiva). [Fiducia e Sicurezza percepita]\n",
      "-               fin_fees <= Esprimi la tua valutazione sulle app/servizi FINTECH per ciascuno dei seguenti fattori (usa una scala da 1 a 5 dove 1 è Molto Negativa e 5 è Molto Positiva). [Costi e Commissioni]\n",
      "-                 fin_ux <= Esprimi la tua valutazione sulle app/servizi FINTECH per ciascuno dei seguenti fattori (usa una scala da 1 a 5 dove 1 è Molto Negativa e 5 è Molto Positiva). [Facilità d'uso (App/Sito Web)]\n",
      "-                imp_app <= Esprimi la tua valutazione sulle app/servizi FINTECH per ciascuno dei seguenti fattori (usa una scala da 1 a 5 dove 1 è Molto Negativa e 5 è Molto Positiva). [Qualità del Servizio Clienti (via chat/email)]\n",
      "-            imp_no_fees <= Quanto sono importanti per te i seguenti fattori nella scelta di un nuovo servizio finanziario (conto carta etc.)? (Usa una scala da 1 a 5 dove 1 è Per Nulla Importante e 5 è Estremamente Importante). [Assenza di costi fissi (canone zero)]\n",
      "-              imp_brand <= Quanto sono importanti per te i seguenti fattori nella scelta di un nuovo servizio finanziario (conto carta etc.)? (Usa una scala da 1 a 5 dove 1 è Per Nulla Importante e 5 è Estremamente Importante). [Solidità e reputazione storica del brand]\n",
      "-         imp_onboarding <= Quanto sono importanti per te i seguenti fattori nella scelta di un nuovo servizio finanziario (conto carta etc.)? (Usa una scala da 1 a 5 dove 1 è Per Nulla Importante e 5 è Estremamente Importante). [Facilità e velocità di apertura del servizio]\n",
      "-             imp_branch <= Quanto sono importanti per te i seguenti fattori nella scelta di un nuovo servizio finanziario (conto carta etc.)? (Usa una scala da 1 a 5 dove 1 è Per Nulla Importante e 5 è Estremamente Importante). [Disponibilità di una filiale fisica vicino a te]\n",
      "-         imp_innovation <= Quanto sono importanti per te i seguenti fattori nella scelta di un nuovo servizio finanziario (conto carta etc.)? (Usa una scala da 1 a 5 dove 1 è Per Nulla Importante e 5 è Estremamente Importante). [Accesso a servizi innovativi (es. criptovalute trading a basso costo)]\n",
      "-           imp_cashback <= Quanto sono importanti per te i seguenti fattori nella scelta di un nuovo servizio finanziario (conto carta etc.)? (Usa una scala da 1 a 5 dove 1 è Per Nulla Importante e 5 è Estremamente Importante). [Promozioni e cashback]\n",
      "-            future_pref <= Pensando ai prossimi 2-3 anni dove prevedi di gestire la maggior parte delle tue finanze personali?\n",
      "\n",
      "Salvato: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\_dataset_parsed_labeled.csv\n"
     ]
    }
   ],
   "source": [
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path(r\"C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PATH_IN = OUT_DIR / \"_dataset_semicolon.csv\"   # <— usa il file riparato\n",
    "df0 = pd.read_csv(PATH_IN)\n",
    "print(\"Loaded:\", PATH_IN, \"shape:\", df0.shape)\n",
    "\n",
    "# === 1) Trova la riga che contiene i VERI header (basato sui testi del questionario)\n",
    "KEYWORDS = [\n",
    "    \"fascia d'età\", \"occupazione\", \"titolo di studio\", \"fonte di reddito\",\n",
    "    \"principale fornitore\", \"conti correnti\", \"IBAN\",\n",
    "    \"banche TRADIZIONALI\", \"Fiducia e Sicurezza\", \"Costi e Commissioni\",\n",
    "    \"Innovazione e Tecnologia\", \"Facilità d'uso\", \"Qualità del Servizio Clienti\",\n",
    "    \"FINTECH\", \"servizi/app Fintech utilizzi\", \"Importanti\", \"importanza\",\n",
    "    \"Assenza di costi fissi\", \"Solidità e reputazione\", \"velocità di apertura\",\n",
    "    \"filiale fisica\", \"app mobile\", \"servizi innovativi\", \"Promozioni e cashback\",\n",
    "    \"prossimi 2-3 anni\", \"gestire la maggior parte delle tue finanze\"\n",
    "]\n",
    "kw_regex = re.compile(\"|\".join([re.escape(k) for k in KEYWORDS]), re.IGNORECASE)\n",
    "\n",
    "def header_score(row_vals):\n",
    "    vals = [str(x) for x in row_vals]\n",
    "    hits = sum(1 for v in vals if kw_regex.search(v or \"\"))\n",
    "    nonnum = sum(1 for v in vals if not re.fullmatch(r\"-?\\d+(?:\\.\\d+)?\", v or \"\"))\n",
    "    return hits*2 + nonnum\n",
    "\n",
    "candidate_rows = min(5, len(df0))\n",
    "scores = [(i, header_score(df0.iloc[i].tolist())) for i in range(candidate_rows)]\n",
    "scores_sorted = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "best_row, best_score = scores_sorted[0]\n",
    "print(\"Header candidate row:\", best_row, \"score:\", best_score, \"scores:\", scores_sorted)\n",
    "\n",
    "# accetta sia \"Column\\d+\" sia \"col_\\d+\"\n",
    "generic_cols = all(re.fullmatch(r\"(?:Column\\d+|col_\\d+)\", str(c)) for c in df0.columns)\n",
    "if generic_cols and best_score >= max(6, len(df0.columns)*0.15):\n",
    "    new_cols = [str(x).strip() for x in df0.iloc[best_row]]\n",
    "    seen, safe_cols = set(), []\n",
    "    for c in new_cols:\n",
    "        base = re.sub(r\"\\s+\", \" \", c or \"Colonna\")\n",
    "        base = base.replace(\":\", \"\").replace(\";\", \"\").replace(\",\", \"\")\n",
    "        key = base\n",
    "        k = 1\n",
    "        while key in seen:\n",
    "            k += 1\n",
    "            key = f\"{base} ({k})\"\n",
    "        seen.add(key)\n",
    "        safe_cols.append(key)\n",
    "    df = df0.iloc[best_row+1:].reset_index(drop=True)\n",
    "    df.columns = safe_cols\n",
    "    print(\"Promossa riga\", best_row, \"a header. Colonne:\", len(df.columns))\n",
    "else:\n",
    "    df = df0.copy()\n",
    "    print(\"Mantengo header esistenti.\")\n",
    "\n",
    "# === 2) Normalizza stringhe\n",
    "for c in df.columns:\n",
    "    if df[c].dtype == object:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# === 3) Mappa colonne -> etichette canoniche\n",
    "PAT = {\n",
    "    # Demografia\n",
    "    \"age_band\": r\"fascia.*et[àa]|18\\s*[-–]\\s*21|22\\s*[-–]\\s*25|26\\s*[-–]\\s*30\",\n",
    "    \"occupation\": r\"\\boccupazione\\b|lavoratore|studente\",\n",
    "    \"education\": r\"(titolo.*studio|laurea|diploma|master)\",\n",
    "    \"income_source\": r\"(fonte.*reddito|sostentamento|supporto.*familiare|part-?time|full-?time|borsa.*studio)\",\n",
    "    # Struttura conto/fornitore\n",
    "    \"primary_provider\": r\"(principale).*(fornitore|istituto|banc[ao])|dove accrediti lo stipendio\",\n",
    "    \"accounts_count\": r\"(conti).*(correnti|iban)|quanti conti\",\n",
    "    # Valutazioni TRAD\n",
    "    \"trad_trust\": r\"(tradizional).*(fiducia|sicurezza)\",\n",
    "    \"trad_fees\": r\"(tradizional).*(costi|commissioni)\",\n",
    "    \"trad_innov\": r\"(tradizional).*(innovazione|tecnologia)\",\n",
    "    \"trad_ux\": r\"(tradizional).*(facilit[aà].*uso|app|sito|usabilit)\",\n",
    "    \"trad_service\": r\"(tradizional).*(servizio|assistenza|filiale|telefonico)\",\n",
    "    # Uso Fintech (tipi di servizio) — multirisposta\n",
    "    \"fintech_services_multi\": r\"(quali).*(servizi|app).*(fintech).*(utilizzi)\",\n",
    "    # Valutazioni FINTECH\n",
    "    \"fin_trust\": r\"(fintech).*(fiducia|sicurezza)\",\n",
    "    \"fin_fees\": r\"(fintech).*(costi|commissioni)\",\n",
    "    \"fin_ux\": r\"(fintech).*(facilit[aà].*uso|app|sito|usabilit)\",\n",
    "    \"fin_service\": r\"(fintech).*(servizio|assistenza|chat|email)\",\n",
    "    # Importanza fattori\n",
    "    \"imp_no_fees\": r\"(assenza).*(costi).*fissi|canone.*zero\",\n",
    "    \"imp_brand\": r\"(solidit[aà]|reputazione).*(brand)\",\n",
    "    \"imp_onboarding\": r\"(facilit[aà]|velocit[aà]).*(apertura)\",\n",
    "    \"imp_branch\": r\"(filiale).*fisica\",\n",
    "    \"imp_app\": r\"(app).*(qualit[aà]|velocit[aà]|design)\",\n",
    "    \"imp_innovation\": r\"(servizi).*(innovativi|criptovalute|trading|basso.*costo)\",\n",
    "    \"imp_cashback\": r\"(promozioni|cashback)\",\n",
    "    # Prospettiva 2–3 anni\n",
    "    \"future_pref\": r\"(prossimi).*(2-?3).*(anni)|maggior.*finanze|prevedi.*gestire\"\n",
    "}\n",
    "\n",
    "def first_match(colnames, pattern):\n",
    "    for c in colnames:\n",
    "        if re.search(pattern, c, flags=re.IGNORECASE):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "rename_map = {}\n",
    "for key, pat in PAT.items():\n",
    "    c = first_match(df.columns, pat)\n",
    "    if c: rename_map[c] = key\n",
    "\n",
    "# Se Column26/col_26 è nel df ed è la lista provider-app, assegniamola se non già mappata\n",
    "if (\"Column26\" in df.columns or \"col_26\" in df.columns) and \"fintech_services_multi\" not in rename_map.values():\n",
    "    if \"Column26\" in df.columns:\n",
    "        rename_map[\"Column26\"] = \"fintech_services_multi\"\n",
    "    else:\n",
    "        rename_map[\"col_26\"] = \"fintech_services_multi\"\n",
    "\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "print(\"\\nMappatura colonne (canonico <= originale):\")\n",
    "for orig, new in rename_map.items():\n",
    "    print(f\"- {new:>22} <= {orig}\")\n",
    "\n",
    "# === 4) Salva\n",
    "OUT_PARSED = OUT_DIR / \"_dataset_parsed_labeled.csv\"\n",
    "df.to_csv(OUT_PARSED, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\nSalvato:\", OUT_PARSED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2e581d",
   "metadata": {},
   "source": [
    "## **STEP 2 — Pulizia & Normalizzazione**\n",
    "**Logica:** (i) correzione di eventuali etichette mal posizionate, (ii) normalizzazione\n",
    "delle **scale Likert a 1–5**, (iii) derivazione **fasce d’età** e **coorti**,\n",
    "(iv) costruzione degli **indici sintetici** (TRAD, FIN, IMPORTANCE) come medie di item disponibili;\n",
    "(v) normalizzazione della preferenza futura.\n",
    "\n",
    "**Output:** `_dataset_clean.csv` con colonne normalizzate e indici.\n",
    "\n",
    "**Next:** lo Step 2B rifinisce eventuali colonne ambigue (es. `imp_app`) leggendo i raw headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8320106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded labeled: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\_dataset_parsed_labeled.csv shape: (1064, 25)\n",
      "Creato 'fin_service' copiando da 'imp_app' (fallback).\n",
      "\n",
      "Indici (media 1–5) — head():\n",
      "   IDX_TRAD  IDX_FIN  IDX_IMPORTANCE\n",
      "0       2.6     3.00        3.285714\n",
      "1       3.2     3.00        2.285714\n",
      "2       3.4     4.00        3.428571\n",
      "3       2.8     2.50        3.714286\n",
      "4       3.2     3.75        3.428571\n",
      "\n",
      "Coverage indici (non-NA counts):\n",
      "IDX_TRAD          1064\n",
      "IDX_FIN           1064\n",
      "IDX_IMPORTANCE    1064\n",
      "dtype: int64\n",
      "\n",
      "Distribuzione 'future_pref_norm' (FIX):\n",
      "future_pref_norm\n",
      "Mix             429\n",
      "Tradizionale    356\n",
      "Fintech         215\n",
      "NS/NA            64\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Salvato CLEAN (FIX): C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\_dataset_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# STEP 2 — FIX + normalizzazione Likert + indici (senza explode in-place)\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\")\n",
    "LAB  = BASE / \"_dataset_parsed_labeled.csv\"    # risultato Step 1\n",
    "SEMICSV = BASE / \"_dataset_semicolon.csv\"      # file con header originali (ci serve per cercare testi)\n",
    "OUT  = BASE / \"_dataset_clean.csv\"\n",
    "\n",
    "df = pd.read_csv(LAB)\n",
    "print(\"Loaded labeled:\", LAB, \"shape:\", df.shape)\n",
    "\n",
    "# ---------- 1) FIX ETICHETTE ----------\n",
    "if \"fin_service\" not in df.columns:\n",
    "    df_raw = pd.read_csv(SEMICSV)\n",
    "    fin_serv_col = None\n",
    "    for c in df_raw.columns:\n",
    "        s = str(c).lower()\n",
    "        if (\"fintech\" in s) and (\"qualità\" in s or \"qualita\" in s) and (\"servizio clienti\" in s or \"assistenza\" in s or \"chat\" in s or \"email\" in s):\n",
    "            fin_serv_col = c\n",
    "            break\n",
    "    if fin_serv_col and fin_serv_col in df.columns and \"imp_app\" in df.columns and fin_serv_col == \"imp_app\":\n",
    "        df = df.rename(columns={\"imp_app\": \"fin_service\"})\n",
    "        print(\"Rinominata 'imp_app' -> 'fin_service' (era la qualità del servizio clienti FINTECH).\")\n",
    "    elif \"imp_app\" in df.columns:\n",
    "        df[\"fin_service\"] = df[\"imp_app\"]\n",
    "        print(\"Creato 'fin_service' copiando da 'imp_app' (fallback).\")\n",
    "\n",
    "if \"imp_app\" not in df.columns:\n",
    "    df_raw = pd.read_csv(SEMICSV)\n",
    "    imp_app_guess = None\n",
    "    for c in df_raw.columns:\n",
    "        s = str(c).lower()\n",
    "        if (\"importante\" in s or \"importanza\" in s) and (\"app\" in s) and (\"mobile\" in s):\n",
    "            imp_app_guess = c\n",
    "            break\n",
    "    if imp_app_guess and imp_app_guess in df.columns:\n",
    "        df = df.rename(columns={imp_app_guess: \"imp_app\"})\n",
    "        print(\"Rinominata colonna IMPORTANZA app -> 'imp_app'.\")\n",
    "    else:\n",
    "        print(\"ATTENZIONE: non ho trovato la colonna 'imp_app' (IMPORTANZA app mobile). Proseguo senza di essa.\")\n",
    "\n",
    "# ---------- 2) Normalizza testi / spazi ----------\n",
    "for c in df.columns:\n",
    "    if df[c].dtype == object:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# ---------- 3) Likert 1–5 ----------\n",
    "LIKERT_MAP = [\n",
    "    (r\"^(?:1)(?:\\D|$)|molto\\s*neg\", 1),\n",
    "    (r\"^(?:2)(?:\\D|$)|poco\\s*(?:pos|imp)\", 2),\n",
    "    (r\"^(?:3)(?:\\D|$)|neutro|n[ée]\", 3),\n",
    "    (r\"^(?:4)(?:\\D|$)|abbastanza|piuttosto\", 4),\n",
    "    (r\"^(?:5)(?:\\D|$)|molto\\s*pos|estremamente\\s*imp|molto\\s*imp\", 5),\n",
    "    (r\"strongly\\s*disagree\", 1),\n",
    "    (r\"\\bdisagree\\b\", 2),\n",
    "    (r\"\\bneutral\\b\", 3),\n",
    "    (r\"\\bagree\\b\", 4),\n",
    "    (r\"strongly\\s*agree\", 5),\n",
    "]\n",
    "def to_likert_safe(s):\n",
    "    s = s.astype(str).str.lower().str.strip()\n",
    "    out = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if out.isna().all():\n",
    "        out = pd.Series([np.nan]*len(s))\n",
    "    for pat, val in LIKERT_MAP:\n",
    "        mask = s.str.contains(pat, regex=True, na=False)\n",
    "        out = out.where(~mask, val)\n",
    "    return pd.to_numeric(out, errors=\"coerce\").clip(1,5)\n",
    "\n",
    "LIKERT_COLS = [\n",
    "    \"trad_trust\",\"trad_fees\",\"trad_innov\",\"trad_ux\",\"trad_service\",\n",
    "    \"fin_trust\",\"fin_fees\",\"fin_ux\",\"fin_service\",\n",
    "    \"imp_no_fees\",\"imp_brand\",\"imp_onboarding\",\"imp_branch\",\"imp_app\",\"imp_innovation\",\"imp_cashback\"\n",
    "]\n",
    "for c in [x for x in LIKERT_COLS if x in df.columns]:\n",
    "    df[c] = to_likert_safe(df[c])\n",
    "\n",
    "# ---------- 4) Fasce d’età → coorte ----------\n",
    "def normalize_age_band(x:str):\n",
    "    x = str(x).lower()\n",
    "    if re.search(r\"18\\s*[-–]\\s*21\", x): return \"18-21\"\n",
    "    if re.search(r\"22\\s*[-–]\\s*25\", x): return \"22-25\"\n",
    "    if re.search(r\"26\\s*[-–]\\s*30\", x): return \"26-30\"\n",
    "    return \"NA\"\n",
    "if \"age_band\" in df.columns:\n",
    "    df[\"age_band_norm\"] = df[\"age_band\"].map(normalize_age_band)\n",
    "else:\n",
    "    df[\"age_band_norm\"] = \"NA\"\n",
    "\n",
    "def band_to_cohort(b):\n",
    "    if b in [\"18-21\",\"22-25\"]: return \"Gen Z 18-25\"\n",
    "    if b == \"26-30\": return \"26-30 (Z/Mix)\"\n",
    "    return \"Altro/NA\"\n",
    "df[\"coorte\"] = df[\"age_band_norm\"].map(band_to_cohort)\n",
    "\n",
    "# ---------- 5) (Niente explode qui) ----------\n",
    "# Le dummies dei servizi Fintech verranno create in 4A in modo robusto.\n",
    "\n",
    "# ---------- 6) Indici compositi (medie 1–5) ----------\n",
    "def mean_if_any(row, cols):\n",
    "    vals = [row[c] for c in cols if c in row.index]\n",
    "    vals = [v for v in vals if pd.notna(v)]\n",
    "    return np.mean(vals) if len(vals) else np.nan\n",
    "\n",
    "df[\"IDX_TRAD\"] = df.apply(lambda r: mean_if_any(r, [\"trad_trust\",\"trad_fees\",\"trad_innov\",\"trad_ux\",\"trad_service\"]), axis=1)\n",
    "df[\"IDX_FIN\"]  = df.apply(lambda r: mean_if_any(r, [\"fin_trust\",\"fin_fees\",\"fin_ux\",\"fin_service\"]), axis=1)\n",
    "df[\"IDX_IMPORTANCE\"] = df.apply(lambda r: mean_if_any(\n",
    "    r, [\"imp_no_fees\",\"imp_brand\",\"imp_onboarding\",\"imp_branch\",\"imp_app\",\"imp_innovation\",\"imp_cashback\"]), axis=1)\n",
    "\n",
    "print(\"\\nIndici (media 1–5) — head():\")\n",
    "print(df[[\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\"]].head())\n",
    "print(\"\\nCoverage indici (non-NA counts):\")\n",
    "print(df[[\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\"]].notna().sum())\n",
    "\n",
    "# ---------- 7) Preferenze future ----------\n",
    "if \"future_pref\" in df.columns:\n",
    "    s = df[\"future_pref\"].astype(str).str.lower()\n",
    "    def map_future(x):\n",
    "        if \"banca tradizionale\" in x: return \"Tradizionale\"\n",
    "        if \"neobanca\" in x or \"fintech\" in x: return \"Fintech\"\n",
    "        if \"mix\" in x: return \"Mix\"\n",
    "        if \"non saprei\" in x: return \"NS/NA\"\n",
    "        return \"Altro/NA\"\n",
    "    df[\"future_pref_norm\"] = s.map(map_future)\n",
    "    print(\"\\nDistribuzione 'future_pref_norm' (FIX):\")\n",
    "    print(df[\"future_pref_norm\"].value_counts(dropna=False))\n",
    "\n",
    "# ---------- 8) Salva ----------\n",
    "df.to_csv(OUT, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\nSalvato CLEAN (FIX):\", OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1c6a4",
   "metadata": {},
   "source": [
    "## **STEP 2B — Rifinitura colonne ambigue + ricalcolo indici**\n",
    "**Logica:** tenta di recuperare con euristiche la vera colonna **`imp_app`** e (se serve) **`fin_service`**\n",
    "consultando gli **header originali**; poi ripete la normalizzazione Likert e il calcolo degli indici.\n",
    "\n",
    "**Output:** aggiorna `_dataset_clean.csv`.\n",
    "\n",
    "**Next:** Step 3 costruirà i KPI descrittivi e le tabelle per età/coorte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f52577b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\_dataset_parsed_labeled.csv (1064, 25) | Raw headers: (1065, 25)\n",
      "Heuristic match for imp_app column in raw headers: None\n",
      "ATTENZIONE: non ho trovato una colonna IMPORTANZA 'app mobile' nei raw headers; mantengo la precedente 'imp_app'.\n",
      "ATTENZIONE: non trovo una colonna valutazione FINTECH 'servizio clienti' nei raw headers; tengo la versione esistente.\n",
      "\n",
      "Salvato CLEAN (aggiornato): C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\_dataset_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# STEP 2B — correzione imp_app e ricalcolo indici (ridotto; mantiene approccio senza explode)\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\")\n",
    "LAB  = BASE / \"_dataset_parsed_labeled.csv\"\n",
    "SEMICSV = BASE / \"_dataset_semicolon.csv\"\n",
    "OUT  = BASE / \"_dataset_clean.csv\"\n",
    "\n",
    "df = pd.read_csv(LAB)\n",
    "df_raw = pd.read_csv(SEMICSV)\n",
    "print(\"Loaded:\", LAB, df.shape, \"| Raw headers:\", df_raw.shape)\n",
    "\n",
    "# 1) Heuristic imp_app dai raw headers\n",
    "imp_app_guess = None\n",
    "for c in df_raw.columns:\n",
    "    s = str(c).lower()\n",
    "    if (\"quanto sono importanti\" in s or \"importante\" in s or \"importanza\" in s) and (\"app\" in s):\n",
    "        imp_app_guess = c\n",
    "        break\n",
    "print(\"Heuristic match for imp_app column in raw headers:\", imp_app_guess)\n",
    "\n",
    "if imp_app_guess and imp_app_guess in df.columns:\n",
    "    df[\"imp_app\"] = df[imp_app_guess]\n",
    "else:\n",
    "    print(\"ATTENZIONE: non ho trovato una colonna IMPORTANZA 'app mobile' nei raw headers; mantengo la precedente 'imp_app'.\")\n",
    "\n",
    "# 2) fin_service dai raw headers se necessario\n",
    "if \"fin_service\" not in df.columns or df[\"fin_service\"].isna().all():\n",
    "    fin_serv_col = None\n",
    "    for c in df_raw.columns:\n",
    "        s = str(c).lower()\n",
    "        if (\"fintech\" in s) and (\"servizio clienti\" in s or \"assistenza\" in s or \"chat\" in s or \"email\" in s):\n",
    "            fin_serv_col = c\n",
    "            break\n",
    "    if fin_serv_col and fin_serv_col in df.columns:\n",
    "        df[\"fin_service\"] = df[fin_serv_col]\n",
    "        print(\"Creato 'fin_service' dai raw headers:\", fin_serv_col)\n",
    "    else:\n",
    "        print(\"ATTENZIONE: non trovo una colonna valutazione FINTECH 'servizio clienti' nei raw headers; tengo la versione esistente.\")\n",
    "\n",
    "# 3) Normalizza Likert + indici\n",
    "LIKERT_MAP = [\n",
    "    (r\"^(?:1)(?:\\D|$)|molto\\s*neg\", 1),\n",
    "    (r\"^(?:2)(?:\\D|$)|poco\\s*(?:pos|imp)\", 2),\n",
    "    (r\"^(?:3)(?:\\D|$)|neutro|n[ée]\", 3),\n",
    "    (r\"^(?:4)(?:\\D|$)|abbastanza|piuttosto\", 4),\n",
    "    (r\"^(?:5)(?:\\D|$)|molto\\s*pos|estremamente\\s*imp|molto\\s*imp\", 5),\n",
    "    (r\"strongly\\s*disagree\", 1),\n",
    "    (r\"\\bdisagree\\b\", 2),\n",
    "    (r\"\\bneutral\\b\", 3),\n",
    "    (r\"\\bagree\\b\", 4),\n",
    "    (r\"strongly\\s*agree\", 5),\n",
    "]\n",
    "def to_likert_safe(s):\n",
    "    s = s.astype(str).str.lower().str.strip()\n",
    "    out = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if out.isna().all():\n",
    "        out = pd.Series([np.nan]*len(s))\n",
    "    for pat, val in LIKERT_MAP:\n",
    "        mask = s.str.contains(pat, regex=True, na=False)\n",
    "        out = out.where(~mask, val)\n",
    "    return pd.to_numeric(out, errors=\"coerce\").clip(1,5)\n",
    "\n",
    "LIKERT_COLS = [\n",
    "    \"trad_trust\",\"trad_fees\",\"trad_innov\",\"trad_ux\",\"trad_service\",\n",
    "    \"fin_trust\",\"fin_fees\",\"fin_ux\",\"fin_service\",\n",
    "    \"imp_no_fees\",\"imp_brand\",\"imp_onboarding\",\"imp_branch\",\"imp_app\",\"imp_innovation\",\"imp_cashback\"\n",
    "]\n",
    "for c in [x for x in LIKERT_COLS if x in df.columns]:\n",
    "    df[c] = to_likert_safe(df[c])\n",
    "\n",
    "def mean_if_any(row, cols):\n",
    "    vals = [row[c] for c in cols if c in row.index]\n",
    "    vals = [v for v in vals if pd.notna(v)]\n",
    "    return np.mean(vals) if len(vals) else np.nan\n",
    "\n",
    "df[\"IDX_TRAD\"] = df.apply(lambda r: mean_if_any(r, [\"trad_trust\",\"trad_fees\",\"trad_innov\",\"trad_ux\",\"trad_service\"]), axis=1)\n",
    "df[\"IDX_FIN\"]  = df.apply(lambda r: mean_if_any(r, [\"fin_trust\",\"fin_fees\",\"fin_ux\",\"fin_service\"]), axis=1)\n",
    "df[\"IDX_IMPORTANCE\"] = df.apply(lambda r: mean_if_any(\n",
    "    r, [\"imp_no_fees\",\"imp_brand\",\"imp_onboarding\",\"imp_branch\",\"imp_app\",\"imp_innovation\",\"imp_cashback\"]), axis=1)\n",
    "\n",
    "# 4) Ricostruisci fasce/coorti se mancano\n",
    "if \"age_band_norm\" not in df.columns and \"age_band\" in df.columns:\n",
    "    def normalize_age_band(x:str):\n",
    "        x = str(x).lower()\n",
    "        if re.search(r\"18\\s*[-–]\\s*21\", x): return \"18-21\"\n",
    "        if re.search(r\"22\\s*[-–]\\s*25\", x): return \"22-25\"\n",
    "        if re.search(r\"26\\s*[-–]\\s*30\", x): return \"26-30\"\n",
    "        return \"NA\"\n",
    "    df[\"age_band_norm\"] = df[\"age_band\"].map(normalize_age_band)\n",
    "\n",
    "if \"coorte\" not in df.columns and \"age_band_norm\" in df.columns:\n",
    "    def band_to_cohort(b):\n",
    "        if b in [\"18-21\",\"22-25\"]: return \"Gen Z 18-25\"\n",
    "        if b == \"26-30\": return \"26-30 (Z/Mix)\"\n",
    "        return \"Altro/NA\"\n",
    "    df[\"coorte\"] = df[\"age_band_norm\"].map(band_to_cohort)\n",
    "\n",
    "# 5) Salva pulito\n",
    "df.to_csv(OUT, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\nSalvato CLEAN (aggiornato):\", OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022826b0",
   "metadata": {},
   "source": [
    "## **STEP 3 — KPI descrittivi e tabelle per fascia/coorte**\n",
    "**Logica:** crea distribuzioni base (età, coorti), **normalizza il provider principale**, calcola\n",
    "medie (1–5) per TRAD/FIN/IMPORTANCE per fascia e costruisce **indici sintetici medi**.\n",
    "\n",
    "**Output:** diversi CSV `kpi_*.csv` e stampe in console. Questi alimentano grafici allo Step 4A.\n",
    "\n",
    "**Next:** lo Step 4A genera le **derivate robuste** e i **grafici** (stacked 100% e barre) a partire da questi KPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35b94eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\_dataset_clean.csv shape: (1064, 30)\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\kpi_age_dist.csv\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\kpi_cohort_dist.csv\n",
      "\n",
      "Distribuzione fasce d'età:\n",
      " age_band_norm\n",
      "26-30    386\n",
      "18-21    342\n",
      "22-25    336\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuzione coorti:\n",
      " coorte\n",
      "Gen Z 18-25      678\n",
      "26-30 (Z/Mix)    386\n",
      "Name: count, dtype: int64\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\kpi_primary_provider_overall.csv\n",
      "\n",
      "Primary provider — overall:\n",
      "                        count  share_%\n",
      "primary_provider_norm                \n",
      "Banca tradizionale       701     65.9\n",
      "Neobanca/Fintech         283     26.6\n",
      "Poste                     80      7.5\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\kpi_primary_provider_by_age_counts.csv\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\kpi_primary_provider_by_age_share.csv\n",
      "\n",
      "Primary provider — per fascia (quote %):\n",
      " primary_provider_norm  Banca tradizionale  Neobanca/Fintech  Poste\n",
      "age_band_norm                                                     \n",
      "18-21                                61.1              31.0    7.9\n",
      "22-25                                61.3              30.4    8.3\n",
      "26-30                                74.1              19.4    6.5\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\kpi_trad_ratings_by_age.csv\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\kpi_fin_ratings_by_age.csv\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\kpi_importance_by_age.csv\n",
      "\n",
      "Medie TRAD per fascia:\n",
      "                trad_trust (mean)  trad_fees (mean)  trad_innov (mean)  \\\n",
      "age_band_norm                                                           \n",
      "18-21                       3.99              2.51               2.80   \n",
      "22-25                       4.00              2.50               2.76   \n",
      "26-30                       4.34              2.50               2.97   \n",
      "\n",
      "               trad_ux (mean)  trad_service (mean)  trad_trust (sd)  \\\n",
      "age_band_norm                                                         \n",
      "18-21                    3.01                 4.15             0.92   \n",
      "22-25                    2.89                 4.11             0.92   \n",
      "26-30                    3.30                 4.42             0.72   \n",
      "\n",
      "               trad_fees (sd)  trad_innov (sd)  trad_ux (sd)  \\\n",
      "age_band_norm                                                  \n",
      "18-21                    0.50             0.68          0.71   \n",
      "22-25                    0.50             0.64          0.68   \n",
      "26-30                    0.52             0.76          0.64   \n",
      "\n",
      "               trad_service (sd)  \n",
      "age_band_norm                     \n",
      "18-21                       0.69  \n",
      "22-25                       0.71  \n",
      "26-30                       0.62  \n",
      "\n",
      "Medie FINTECH per fascia:\n",
      "                fin_trust (mean)  fin_fees (mean)  fin_ux (mean)  \\\n",
      "age_band_norm                                                     \n",
      "18-21                      2.99             4.49           4.51   \n",
      "22-25                      3.00             4.54           4.58   \n",
      "26-30                      2.81             4.49           4.18   \n",
      "\n",
      "               fin_trust (sd)  fin_fees (sd)  fin_ux (sd)  \n",
      "age_band_norm                                              \n",
      "18-21                    0.92           0.52         0.78  \n",
      "22-25                    0.93           0.52         0.74  \n",
      "26-30                    0.88           0.53         0.86  \n",
      "\n",
      "Importanza fattori per fascia:\n",
      "                imp_no_fees (mean)  imp_brand (mean)  imp_onboarding (mean)  \\\n",
      "age_band_norm                                                                \n",
      "18-21                        5.00              3.72                   3.98   \n",
      "22-25                        4.60              3.66                   4.10   \n",
      "26-30                        4.34              4.24                   3.69   \n",
      "\n",
      "               imp_branch (mean)  imp_app (mean)  imp_innovation (mean)  \\\n",
      "age_band_norm                                                             \n",
      "18-21                       3.06            3.13                   3.03   \n",
      "22-25                       3.08            3.24                   3.25   \n",
      "26-30                       3.63            3.10                   2.49   \n",
      "\n",
      "               imp_cashback (mean)  imp_no_fees (sd)  imp_brand (sd)  \\\n",
      "age_band_norm                                                          \n",
      "18-21                         3.94              0.05            1.32   \n",
      "22-25                         3.64              0.73            1.25   \n",
      "26-30                         3.36              0.82            0.93   \n",
      "\n",
      "               imp_onboarding (sd)  imp_branch (sd)  imp_app (sd)  \\\n",
      "age_band_norm                                                       \n",
      "18-21                         0.72             1.56          0.75   \n",
      "22-25                         0.76             1.57          0.78   \n",
      "26-30                         0.66             1.20          0.79   \n",
      "\n",
      "               imp_innovation (sd)  imp_cashback (sd)  \n",
      "age_band_norm                                          \n",
      "18-21                         1.29               0.85  \n",
      "22-25                         1.27               1.01  \n",
      "26-30                         1.27               1.08  \n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\kpi_indices_by_age.csv\n",
      "\n",
      "Indici sintetici (1–5) per fascia:\n",
      "                IDX_TRAD  IDX_FIN  IDX_IMPORTANCE\n",
      "age_band_norm                                   \n",
      "18-21              3.29     4.00            3.70\n",
      "22-25              3.25     4.04            3.65\n",
      "26-30              3.51     3.83            3.55\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\kpi_future_pref_overall.csv\n",
      "\n",
      "Preferenza futura — overall:\n",
      "                   count  share_%\n",
      "future_pref_norm                \n",
      "Mix                 429     40.3\n",
      "Tradizionale        356     33.5\n",
      "Fintech             215     20.2\n",
      "NS/NA                64      6.0\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\kpi_future_pref_by_age_counts.csv\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\kpi_future_pref_by_age_share.csv\n",
      "\n",
      "Preferenza futura — per fascia (quote %):\n",
      " future_pref_norm  Fintech   Mix  NS/NA  Tradizionale\n",
      "age_band_norm                                       \n",
      "18-21                22.5  38.3    5.8          33.3\n",
      "22-25                22.0  40.5    4.8          32.7\n",
      "26-30                16.6  42.0    7.3          34.2\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\kpi_indices_by_future_pref.csv\n",
      "\n",
      "Indici per preferenza futura:\n",
      "                   IDX_TRAD  IDX_FIN  IDX_IMPORTANCE\n",
      "future_pref_norm                                   \n",
      "Fintech               3.14     4.24            3.61\n",
      "Mix                   3.36     3.93            3.60\n",
      "NS/NA                 3.40     3.89            3.64\n",
      "Tradizionale          3.47     3.81            3.67\n",
      "\n",
      "[Step 3 OK] CSV salvati in: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\n"
     ]
    }
   ],
   "source": [
    "# STEP 3 (fix) — KPI descrittivi + per fascia/coorte (senza errori di pivot)\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\")\n",
    "IN   = BASE / \"_dataset_clean.csv\"\n",
    "\n",
    "df = pd.read_csv(IN)\n",
    "print(\"Loaded:\", IN, \"shape:\", df.shape)\n",
    "\n",
    "# 0) Dedup nomi colonna (tieni la prima occorrenza)\n",
    "dup_mask = pd.Index(df.columns).duplicated(keep='first')\n",
    "if dup_mask.any():\n",
    "    dups = list(pd.Index(df.columns)[dup_mask])\n",
    "    print(\"ATTENZIONE: colonne duplicate rimosse:\", dups)\n",
    "    df = df.loc[:, ~pd.Index(df.columns).duplicated(keep='first')]\n",
    "\n",
    "def save_table(df_, name):\n",
    "    path = BASE / f\"{name}.csv\"\n",
    "    df_.to_csv(path, index=True if isinstance(df_, pd.DataFrame) else False, encoding=\"utf-8-sig\")\n",
    "    print(\"Saved:\", path)\n",
    "\n",
    "# A) Distribuzioni base\n",
    "age_dist = df[\"age_band_norm\"].value_counts(dropna=False).rename(\"count\")\n",
    "cohort_dist = df[\"coorte\"].value_counts(dropna=False).rename(\"count\")\n",
    "save_table(age_dist, \"kpi_age_dist\")\n",
    "save_table(cohort_dist, \"kpi_cohort_dist\")\n",
    "print(\"\\nDistribuzione fasce d'età:\\n\", age_dist)\n",
    "print(\"\\nDistribuzione coorti:\\n\", cohort_dist)\n",
    "\n",
    "# B) Provider principale (normalizzato)\n",
    "if \"primary_provider\" in df.columns:\n",
    "    s = df[\"primary_provider\"].astype(str).str.lower()\n",
    "    def map_provider(x):\n",
    "        if \"tradizionale\" in x: return \"Banca tradizionale\"\n",
    "        if \"neobanca\" in x or \"fintech\" in x: return \"Neobanca/Fintech\"\n",
    "        if \"online\" in x: return \"Banca online (gruppo trad.)\"\n",
    "        if \"posta\" in x: return \"Poste\"\n",
    "        return \"Altro/NA\"\n",
    "    df[\"primary_provider_norm\"] = s.map(map_provider)\n",
    "\n",
    "    prov_overall = df[\"primary_provider_norm\"].value_counts().to_frame(\"count\")\n",
    "    prov_overall[\"share_%\"] = (prov_overall[\"count\"]/prov_overall[\"count\"].sum()*100).round(1)\n",
    "    save_table(prov_overall, \"kpi_primary_provider_overall\")\n",
    "    print(\"\\nPrimary provider — overall:\\n\", prov_overall)\n",
    "\n",
    "    prov_by_age = (df.pivot_table(index=\"age_band_norm\",\n",
    "                                  columns=\"primary_provider_norm\",\n",
    "                                  aggfunc=\"size\", fill_value=0))\n",
    "    prov_by_age_share = (prov_by_age.div(prov_by_age.sum(axis=1), axis=0)*100).round(1)\n",
    "    save_table(prov_by_age, \"kpi_primary_provider_by_age_counts\")\n",
    "    save_table(prov_by_age_share, \"kpi_primary_provider_by_age_share\")\n",
    "    print(\"\\nPrimary provider — per fascia (quote %):\\n\", prov_by_age_share)\n",
    "\n",
    "# C) Tipi Fintech — verranno gestiti in 4A (qui opzionale)\n",
    "\n",
    "# D) Medie valutazioni (1–5) per fascia\n",
    "trad_cols = [c for c in [\"trad_trust\",\"trad_fees\",\"trad_innov\",\"trad_ux\",\"trad_service\"] if c in df.columns]\n",
    "fin_cols  = [c for c in [\"fin_trust\",\"fin_fees\",\"fin_ux\",\"fin_service\"] if c in df.columns]\n",
    "imp_cols  = [c for c in [\"imp_no_fees\",\"imp_brand\",\"imp_onboarding\",\"imp_branch\",\"imp_app\",\"imp_innovation\",\"imp_cashback\"] if c in df.columns]\n",
    "\n",
    "def mean_sd_table(cols, by=None):\n",
    "    if not cols: return pd.DataFrame()\n",
    "    if by:\n",
    "        m = df.groupby(by)[cols].mean().round(2)\n",
    "        s = df.groupby(by)[cols].std().round(2)\n",
    "        return m.add_suffix(\" (mean)\").join(s.add_suffix(\" (sd)\"))\n",
    "    else:\n",
    "        m = df[cols].mean().to_frame(\"mean\").round(2)\n",
    "        s = df[cols].std().to_frame(\"sd\").round(2)\n",
    "        return m.join(s)\n",
    "\n",
    "trad_by_age = mean_sd_table(trad_cols, by=\"age_band_norm\")\n",
    "fin_by_age  = mean_sd_table(fin_cols,  by=\"age_band_norm\")\n",
    "imp_by_age  = mean_sd_table(imp_cols,  by=\"age_band_norm\")\n",
    "save_table(trad_by_age, \"kpi_trad_ratings_by_age\")\n",
    "save_table(fin_by_age,  \"kpi_fin_ratings_by_age\")\n",
    "save_table(imp_by_age,  \"kpi_importance_by_age\")\n",
    "print(\"\\nMedie TRAD per fascia:\\n\", trad_by_age)\n",
    "print(\"\\nMedie FINTECH per fascia:\\n\", fin_by_age)\n",
    "print(\"\\nImportanza fattori per fascia:\\n\", imp_by_age)\n",
    "\n",
    "# Indici sintetici\n",
    "idx_by_age = df.groupby(\"age_band_norm\")[[\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\"]].mean().round(2)\n",
    "save_table(idx_by_age, \"kpi_indices_by_age\")\n",
    "print(\"\\nIndici sintetici (1–5) per fascia:\\n\", idx_by_age)\n",
    "\n",
    "# E) Preferenza futura\n",
    "if \"future_pref\" in df.columns:\n",
    "    s = df[\"future_pref\"].astype(str).str.lower()\n",
    "    def map_future(x):\n",
    "        if \"banca tradizionale\" in x: return \"Tradizionale\"\n",
    "        if \"neobanca\" in x or \"fintech\" in x: return \"Fintech\"\n",
    "        if \"mix\" in x: return \"Mix\"\n",
    "        if \"non saprei\" in x: return \"NS/NA\"\n",
    "        return \"Altro/NA\"\n",
    "    df[\"future_pref_norm\"] = s.map(map_future)\n",
    "\n",
    "    fp_overall = df[\"future_pref_norm\"].value_counts().to_frame(\"count\")\n",
    "    fp_overall[\"share_%\"] = (fp_overall[\"count\"]/fp_overall[\"count\"].sum()*100).round(1)\n",
    "    save_table(fp_overall, \"kpi_future_pref_overall\")\n",
    "    print(\"\\nPreferenza futura — overall:\\n\", fp_overall)\n",
    "\n",
    "    fp_by_age = (df.pivot_table(index=\"age_band_norm\",\n",
    "                                columns=\"future_pref_norm\",\n",
    "                                aggfunc=\"size\", fill_value=0))\n",
    "    fp_by_age_share = (fp_by_age.div(fp_by_age.sum(axis=1), axis=0)*100).round(1)\n",
    "    save_table(fp_by_age, \"kpi_future_pref_by_age_counts\")\n",
    "    save_table(fp_by_age_share, \"kpi_future_pref_by_age_share\")\n",
    "    print(\"\\nPreferenza futura — per fascia (quote %):\\n\", fp_by_age_share)\n",
    "\n",
    "# F) Indici per preferenza futura\n",
    "if \"future_pref_norm\" in df.columns:\n",
    "    idx_by_future = df.groupby(\"future_pref_norm\")[[\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\"]].mean().round(2)\n",
    "    save_table(idx_by_future, \"kpi_indices_by_future_pref\")\n",
    "    print(\"\\nIndici per preferenza futura:\\n\", idx_by_future)\n",
    "\n",
    "print(\"\\n[Step 3 OK] CSV salvati in:\", BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae3ca64",
   "metadata": {},
   "source": [
    "## **STEP 4A — Derivate robuste + Grafici**\n",
    "**Logica:** crea variabili **normalizzate** (`*_norm`), dummies di **tipi Fintech** da multirisposta,\n",
    "salva una versione **arricchita** del dataset e produce i **grafici** (bar + 100% stacked).\n",
    "\n",
    "**Output:** `_dataset_clean_enriched.csv` + immagini in `img/`:\n",
    "- `primary_provider_overall.png`, `primary_provider_by_age_100pct.png`\n",
    "- `IDX_TRAD_by_age.png`, `IDX_FIN_by_age.png`, `IDX_IMPORTANCE_by_age.png`\n",
    "- `future_pref_overall.png`, `future_pref_by_age_100pct.png`\n",
    "\n",
    "**Next:** lo Step 4B esegue test statistici (χ² e ANOVA/Kruskal) coerenti con questi KPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a31b4327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\_dataset_clean.csv shape: (1064, 30)\n",
      "CREATA: primary_provider_norm\n",
      "CREATA: future_pref_norm\n",
      "CREATI dummies:  ['use_conti_pagamenti', 'use_invest_trading', 'use_risparmio_budgeting', 'use_credito_bnpl', 'use_nessuno']\n",
      "Saved long table: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\fintech_types_long.csv\n",
      "Saved enriched: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\_dataset_clean_enriched.csv\n",
      "Saved plot: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\img\\primary_provider_overall.png\n",
      "Saved plot: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\img\\primary_provider_by_age_100pct.png\n",
      "Saved plot: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\img\\IDX_TRAD_by_age.png\n",
      "Saved plot: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\img\\IDX_FIN_by_age.png\n",
      "Saved plot: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\img\\IDX_IMPORTANCE_by_age.png\n",
      "Saved plot: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\img\\future_pref_overall.png\n",
      "Saved plot: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\img\\future_pref_by_age_100pct.png\n"
     ]
    }
   ],
   "source": [
    "# STEP 4A — derivate robuste + grafici (con pattern non-catturanti)\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\")\n",
    "IN   = BASE / \"_dataset_clean.csv\"\n",
    "df   = pd.read_csv(IN)\n",
    "print(\"Loaded:\", IN, \"shape:\", df.shape)\n",
    "\n",
    "# 0) Derivate \"norm\"\n",
    "if \"primary_provider_norm\" not in df.columns and \"primary_provider\" in df.columns:\n",
    "    s = df[\"primary_provider\"].astype(str).str.lower()\n",
    "    def map_provider(x):\n",
    "        if \"tradizionale\" in x: return \"Banca tradizionale\"\n",
    "        if \"neobanca\" in x or \"fintech\" in x: return \"Neobanca/Fintech\"\n",
    "        if \"online\" in x: return \"Banca online (gruppo trad.)\"\n",
    "        if \"posta\" in x: return \"Poste\"\n",
    "        return \"Altro/NA\"\n",
    "    df[\"primary_provider_norm\"] = s.map(map_provider)\n",
    "    print(\"CREATA: primary_provider_norm\")\n",
    "\n",
    "if \"future_pref_norm\" not in df.columns and \"future_pref\" in df.columns:\n",
    "    s = df[\"future_pref\"].astype(str).str.lower()\n",
    "    def map_future(x):\n",
    "        if \"banca tradizionale\" in x: return \"Tradizionale\"\n",
    "        if \"neobanca\" in x or \"fintech\" in x: return \"Fintech\"\n",
    "        if \"mix\" in x: return \"Mix\"\n",
    "        if \"non saprei\" in x: return \"NS/NA\"\n",
    "        return \"Altro/NA\"\n",
    "    df[\"future_pref_norm\"] = s.map(map_future)\n",
    "    print(\"CREATA: future_pref_norm\")\n",
    "\n",
    "# 1) Tipi Fintech — dummies per riga (no explode in-place)\n",
    "types_col = \"fintech_services_multi\"\n",
    "TYPES = {\n",
    "    \"conti_pagamenti\": r\"(?:conti|pagamenti)\",\n",
    "    \"invest_trading\":  r\"(?:invest|trading)\",\n",
    "    \"risparmio_budgeting\": r\"(?:risparm|budget)\",\n",
    "    \"credito_bnpl\":    r\"(?:credito|buy now|bnpl|klarna|scalapay|paga in 3)\",\n",
    "    \"nessuno\":         r\"(?:nessuno)\"\n",
    "}\n",
    "if types_col in df.columns:\n",
    "    s = (df[types_col].astype(str)\n",
    "                      .str.lower()\n",
    "                      .str.replace(r\"[|/]\", \",\", regex=True)\n",
    "                      .str.replace(\";\", \",\")\n",
    "                      .str.replace(\"\\t\", \",\"))\n",
    "    for t, pat in TYPES.items():\n",
    "        df[f\"use_{t}\"] = s.str.contains(pat, regex=True, na=False).astype(int)\n",
    "    print(\"CREATI dummies: \", [c for c in df.columns if c.startswith(\"use_\")])\n",
    "\n",
    "    # Tabella long separata (opzionale)\n",
    "    types_long = (s.str.split(\",\").explode().str.strip().replace({\"\": np.nan}).dropna())\n",
    "    def map_type(x):\n",
    "        if re.search(TYPES[\"conti_pagamenti\"], x): return \"conti_pagamenti\"\n",
    "        if re.search(TYPES[\"invest_trading\"], x):  return \"invest_trading\"\n",
    "        if re.search(TYPES[\"risparmio_budgeting\"], x): return \"risparmio_budgeting\"\n",
    "        if re.search(TYPES[\"credito_bnpl\"], x):    return \"credito_bnpl\"\n",
    "        if re.search(TYPES[\"nessuno\"], x):         return \"nessuno\"\n",
    "        return x\n",
    "    types_long = types_long.map(map_type)\n",
    "    types_long_df = pd.DataFrame({\"fintech_type\": types_long})\n",
    "    OUT_TYPES = BASE / \"fintech_types_long.csv\"\n",
    "    types_long_df.to_csv(OUT_TYPES, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"Saved long table:\", OUT_TYPES)\n",
    "else:\n",
    "    print(\"ATTENZIONE: colonna 'fintech_services_multi' non presente. Salto i dummies/types.\")\n",
    "\n",
    "# 2) Salva versione arricchita\n",
    "ENR = BASE / \"_dataset_clean_enriched.csv\"\n",
    "df.to_csv(ENR, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved enriched:\", ENR)\n",
    "\n",
    "# 3) Grafici (no seaborn, no colori fissati)\n",
    "OUTIMG = BASE / \"img\"\n",
    "OUTIMG.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def savefig(name):\n",
    "    p = OUTIMG / f\"{name}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(p, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved plot:\", p)\n",
    "\n",
    "# Primary provider — overall\n",
    "if \"primary_provider_norm\" in df.columns:\n",
    "    prov = df[\"primary_provider_norm\"].value_counts().sort_values(ascending=False)\n",
    "    plt.figure()\n",
    "    prov.plot(kind=\"bar\")\n",
    "    plt.title(\"Primary provider — overall\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"N\")\n",
    "    savefig(\"primary_provider_overall\")\n",
    "\n",
    "# Primary provider — 100% stacked per fascia\n",
    "if \"primary_provider_norm\" in df.columns and \"age_band_norm\" in df.columns:\n",
    "    tab = (df.pivot_table(index=\"age_band_norm\", columns=\"primary_provider_norm\",\n",
    "                          aggfunc=\"size\", fill_value=0)\n",
    "             .reindex([\"18-21\",\"22-25\",\"26-30\"]))\n",
    "    tab_pct = tab.div(tab.sum(axis=1), axis=0)*100\n",
    "\n",
    "    plt.figure()\n",
    "    bottom = np.zeros(len(tab_pct))\n",
    "    x = np.arange(len(tab_pct.index))\n",
    "    for col in tab_pct.columns:\n",
    "        plt.bar(x, tab_pct[col].values, bottom=bottom, label=col)\n",
    "        bottom += tab_pct[col].values\n",
    "    plt.xticks(x, tab_pct.index)\n",
    "    plt.ylabel(\"%\")\n",
    "    plt.title(\"Primary provider — per fascia (100% stacked)\")\n",
    "    plt.legend(frameon=False)\n",
    "    savefig(\"primary_provider_by_age_100pct\")\n",
    "\n",
    "# Indici sintetici per fascia\n",
    "if \"age_band_norm\" in df.columns:\n",
    "    idx = (df.groupby(\"age_band_norm\")[[\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\"]]\n",
    "             .mean().reindex([\"18-21\",\"22-25\",\"26-30\"]))\n",
    "    for col in [\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\"]:\n",
    "        plt.figure()\n",
    "        idx[col].plot(kind=\"bar\")\n",
    "        plt.ylim(1,5)\n",
    "        plt.title(f\"{col} — media per fascia (1–5)\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"Media (1–5)\")\n",
    "        savefig(f\"{col}_by_age\")\n",
    "\n",
    "# Preferenza futura — overall e per fascia\n",
    "if \"future_pref_norm\" in df.columns:\n",
    "    fp = df[\"future_pref_norm\"].value_counts().sort_values(ascending=False)\n",
    "    plt.figure()\n",
    "    fp.plot(kind=\"bar\")\n",
    "    plt.title(\"Preferenza futura — overall\")\n",
    "    plt.ylabel(\"N\")\n",
    "    plt.xlabel(\"\")\n",
    "    savefig(\"future_pref_overall\")\n",
    "\n",
    "if \"future_pref_norm\" in df.columns and \"age_band_norm\" in df.columns:\n",
    "    fp_tab = (df.pivot_table(index=\"age_band_norm\", columns=\"future_pref_norm\",\n",
    "                             aggfunc=\"size\", fill_value=0)\n",
    "                .reindex([\"18-21\",\"22-25\",\"26-30\"]))\n",
    "    fp_pct = fp_tab.div(fp_tab.sum(axis=1), axis=0)*100\n",
    "\n",
    "    plt.figure()\n",
    "    bottom = np.zeros(len(fp_pct))\n",
    "    x = np.arange(len(fp_pct.index))\n",
    "    for col in fp_pct.columns:\n",
    "        plt.bar(x, fp_pct[col].values, bottom=bottom, label=col)\n",
    "        bottom += fp_pct[col].values\n",
    "    plt.xticks(x, fp_pct.index)\n",
    "    plt.ylabel(\"%\")\n",
    "    plt.title(\"Preferenza futura — per fascia (100% stacked)\")\n",
    "    plt.legend(frameon=False)\n",
    "    savefig(\"future_pref_by_age_100pct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9674a3c",
   "metadata": {},
   "source": [
    "## **STEP 4B — Test statistici descrittivi**\n",
    "**Logica:** verifica dipendenze tra **età × provider** con χ² e differenze tra fasce sugli **indici**\n",
    "con ANOVA (fallback Kruskal se necessario).\n",
    "\n",
    "**Output:** statistiche e p-value stampati a console.\n",
    "\n",
    "**Next:** lo Step 4C sintetizza **effect size** (Cramér’s V, η²) e **post-hoc** (Tukey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edbbc6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded enriched: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\_dataset_clean_enriched.csv shape: (1064, 37)\n",
      "\n",
      "[CHI2] age_band_norm × primary_provider_norm\n",
      "chi2= 18.84 dof= 4 p= 0.0008437658582425696\n",
      "Crosstab:\n",
      " primary_provider_norm  Banca tradizionale  Neobanca/Fintech  Poste\n",
      "age_band_norm                                                     \n",
      "18-21                                 209               106     27\n",
      "22-25                                 206               102     28\n",
      "26-30                                 286                75     25\n",
      "\n",
      "[ANOVA F] IDX_TRAD per fascia -> stat=47.85 p=1.2740729591302122e-20\n",
      "\n",
      "[ANOVA F] IDX_FIN per fascia -> stat=18.56 p=1.1895618572660457e-08\n",
      "\n",
      "[ANOVA F] IDX_IMPORTANCE per fascia -> stat=14.4 p=6.734412842741012e-07\n"
     ]
    }
   ],
   "source": [
    "# STEP 4B — Test statistici descrittivi (χ² + ANOVA/Kruskal)\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\")\n",
    "IN   = BASE / \"_dataset_clean_enriched.csv\"\n",
    "df   = pd.read_csv(IN)\n",
    "print(\"Loaded enriched:\", IN, \"shape:\", df.shape)\n",
    "\n",
    "# 1) χ²: age_band_norm × primary_provider_norm\n",
    "if \"age_band_norm\" in df.columns and \"primary_provider_norm\" in df.columns:\n",
    "    ct = pd.crosstab(df[\"age_band_norm\"], df[\"primary_provider_norm\"])\n",
    "    chi2, p, dof, exp = stats.chi2_contingency(ct)\n",
    "    print(\"\\n[CHI2] age_band_norm × primary_provider_norm\")\n",
    "    print(\"chi2=\", round(chi2,2), \"dof=\", dof, \"p=\", p)\n",
    "    print(\"Crosstab:\\n\", ct)\n",
    "else:\n",
    "    print(\"\\n[CHI2] SKIP: manca age_band_norm o primary_provider_norm\")\n",
    "\n",
    "# 2) ANOVA/Kruskal su indici per fascia\n",
    "for col in [\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\"]:\n",
    "    if col in df.columns and \"age_band_norm\" in df.columns:\n",
    "        groups = [g.dropna().values for _,g in df.groupby(\"age_band_norm\")[col]]\n",
    "        try:\n",
    "            f,pv = stats.f_oneway(*groups)\n",
    "            print(f\"\\n[ANOVA F] {col} per fascia -> stat={round(f,2)} p={pv}\")\n",
    "        except Exception:\n",
    "            h,pv = stats.kruskal(*groups)\n",
    "            print(f\"\\n[Kruskal H] {col} per fascia -> stat={round(h,2)} p={pv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d0e1b",
   "metadata": {},
   "source": [
    "## **STEP 4C — Effect size + post-hoc**\n",
    "**Logica:** quantifica l’intensità delle associazioni:\n",
    "- **Cramér’s V** per `age × provider`\n",
    "- **Eta-squared (η²)** per ANOVA sugli indici\n",
    "- **Tukey HSD** per confronti a coppie\n",
    "\n",
    "**Output:** print degli indici + CSV `tukey_*.csv` in `kpi_effects/`.\n",
    "\n",
    "**Next:** Step 4D passa alla **MNLogit** per interpretare i driver della preferenza futura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b43ed7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cramér's V = 0.094  (chi2=18.84, dof=4, p=0.000844)\n",
      "\n",
      "IDX_TRAD: F=47.85, p=1.274e-20, eta^2=0.083  (n=1064)\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05\n",
      "===================================================\n",
      "group1 group2 meandiff p-adj   lower  upper  reject\n",
      "---------------------------------------------------\n",
      " 18-21  22-25  -0.0424 0.3173 -0.1111 0.0264  False\n",
      " 18-21  26-30   0.2143    0.0  0.1478 0.2807   True\n",
      " 22-25  26-30   0.2567    0.0  0.1899 0.3234   True\n",
      "---------------------------------------------------\n",
      "\n",
      "IDX_FIN: F=18.56, p=1.190e-08, eta^2=0.034  (n=1064)\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "====================================================\n",
      "group1 group2 meandiff p-adj   lower   upper  reject\n",
      "----------------------------------------------------\n",
      " 18-21  22-25   0.0377 0.5875 -0.0523  0.1276  False\n",
      " 18-21  26-30  -0.1726    0.0 -0.2595 -0.0857   True\n",
      " 22-25  26-30  -0.2103    0.0 -0.2976 -0.1229   True\n",
      "----------------------------------------------------\n",
      "\n",
      "IDX_IMPORTANCE: F=14.40, p=6.734e-07, eta^2=0.026  (n=1064)\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "====================================================\n",
      "group1 group2 meandiff p-adj   lower   upper  reject\n",
      "----------------------------------------------------\n",
      " 18-21  22-25  -0.0429 0.2942 -0.1102  0.0245  False\n",
      " 18-21  26-30   -0.144    0.0 -0.2091 -0.0789   True\n",
      " 22-25  26-30  -0.1011 0.0009 -0.1665 -0.0357   True\n",
      "----------------------------------------------------\n",
      "\n",
      "[4C OK] Salvati i post-hoc in: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\kpi_effects\n"
     ]
    }
   ],
   "source": [
    "## 4C — Effect size + post-hoc per §4.4\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import statsmodels.stats.multicomp as mc\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\")\n",
    "IN   = BASE / \"_dataset_clean_enriched.csv\"\n",
    "df   = pd.read_csv(IN)\n",
    "\n",
    "OUT_DIR = BASE / \"kpi_effects\"\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# --- 1) Cramér's V per age x provider ---\n",
    "ct = pd.crosstab(df[\"age_band_norm\"], df[\"primary_provider_norm\"])\n",
    "chi2, p, dof, exp = stats.chi2_contingency(ct)\n",
    "n = ct.to_numpy().sum()\n",
    "k = min(ct.shape[0]-1, ct.shape[1]-1)\n",
    "cramers_v = np.sqrt(chi2/(n*k))\n",
    "print(f\"Cramér's V = {cramers_v:.3f}  (chi2={chi2:.2f}, dof={dof}, p={p:.6f})\")\n",
    "\n",
    "# --- 2) Eta-squared (ANOVA) + Tukey HSD per ciascun indice ---\n",
    "def anova_eta_tukey(col):\n",
    "    d = df[[\"age_band_norm\", col]].dropna()\n",
    "    groups = [g[col].values for _, g in d.groupby(\"age_band_norm\")]\n",
    "    F, pval = stats.f_oneway(*groups)\n",
    "    k_groups = d[\"age_band_norm\"].nunique()\n",
    "    n_tot = len(d)\n",
    "    df_between = k_groups - 1\n",
    "    df_within  = n_tot - k_groups\n",
    "    eta2 = (F*df_between)/((F*df_between)+df_within)\n",
    "    print(f\"\\n{col}: F={F:.2f}, p={pval:.3e}, eta^2={eta2:.3f}  (n={n_tot})\")\n",
    "    comp = mc.MultiComparison(d[col], d[\"age_band_norm\"])\n",
    "    tuk = comp.tukeyhsd(alpha=0.05)\n",
    "    print(tuk.summary())\n",
    "    res = pd.DataFrame(tuk._results_table.data[1:], columns=tuk._results_table.data[0])\n",
    "    res.to_csv(OUT_DIR / f\"tukey_{col}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "for col in [\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\"]:\n",
    "    anova_eta_tukey(col)\n",
    "\n",
    "print(\"\\n[4C OK] Salvati i post-hoc in:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb48e83",
   "metadata": {},
   "source": [
    "## **STEP 4D — MNLogit (esplicativa)**\n",
    "**Logica:** modello **multinomiale** con outcome a 3 classi (Trad/ Mix/ Fintech) spiegato da\n",
    "indici **IDX_TRAD, IDX_FIN, IDX_IMPORTANCE** e **fasce d’età** (baseline 26–30).\n",
    "Riporta **coef**, **RRR**, **AME** e **pseudo-R²**.\n",
    "\n",
    "**Output:** CSV `mnlogit_coef.csv`, `mnlogit_rrr.csv`, `mnlogit_mfx_overall.csv`.\n",
    "\n",
    "**Next:** Step 4E calcola **VIF** sullo stesso design per controllare collinearità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d777676a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded enriched: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\_dataset_clean_enriched.csv shape: (1064, 37)\n",
      "\n",
      "Dtypes X (verifica):\n",
      "const             float64\n",
      "IDX_TRAD          float64\n",
      "IDX_FIN           float64\n",
      "IDX_IMPORTANCE    float64\n",
      "22-25             float64\n",
      "18-21             float64\n",
      "dtype: object\n",
      "\n",
      "[MNLogit] Preferenza futura ~ indici + fascia\n",
      "                          MNLogit Regression Results                          \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  937\n",
      "Model:                        MNLogit   Df Residuals:                      925\n",
      "Method:                           MLE   Df Model:                           10\n",
      "Date:                Wed, 10 Sep 2025   Pseudo R-squ.:                 0.06420\n",
      "Time:                        01:12:59   Log-Likelihood:                -927.09\n",
      "converged:                       True   LL-Null:                       -990.69\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.741e-22\n",
      "==================================================================================\n",
      "           y=1       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "const              2.5677      1.309      1.962      0.050       0.003       5.132\n",
      "IDX_TRAD          -0.5929      0.220     -2.693      0.007      -1.024      -0.161\n",
      "IDX_FIN            0.3822      0.171      2.232      0.026       0.047       0.718\n",
      "IDX_IMPORTANCE    -0.4624      0.201     -2.296      0.022      -0.857      -0.068\n",
      "22-25              0.0038      0.183      0.021      0.984      -0.356       0.363\n",
      "18-21             -0.4004      0.183     -2.186      0.029      -0.759      -0.041\n",
      "----------------------------------------------------------------------------------\n",
      "           y=2       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "const             -0.5895      1.705     -0.346      0.730      -3.931       2.752\n",
      "IDX_TRAD          -1.4116      0.281     -5.027      0.000      -1.962      -0.861\n",
      "IDX_FIN            1.2866      0.223      5.774      0.000       0.850       1.723\n",
      "IDX_IMPORTANCE    -0.1014      0.256     -0.396      0.692      -0.604       0.401\n",
      "22-25              0.0009      0.238      0.004      0.997      -0.465       0.467\n",
      "18-21             -0.0444      0.233     -0.190      0.849      -0.502       0.413\n",
      "==================================================================================\n",
      "\n",
      "McFadden pseudo-R^2: 0.064\n",
      "\n",
      "Relative Risk Ratios (exp(coef)):\n",
      "                     0      1\n",
      "const           13.035  0.555\n",
      "IDX_TRAD         0.553  0.244\n",
      "IDX_FIN          1.466  3.621\n",
      "IDX_IMPORTANCE   0.630  0.904\n",
      "22-25            1.004  1.001\n",
      "18-21            0.670  0.957\n",
      "\n",
      "Marginal effects (overall):\n",
      "       MNLogit Marginal Effects      \n",
      "=====================================\n",
      "Dep. Variable:                      y\n",
      "Method:                          dydx\n",
      "At:                           overall\n",
      "==================================================================================\n",
      "           y=0      dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "IDX_TRAD           0.1747      0.043      4.089      0.000       0.091       0.258\n",
      "IDX_FIN           -0.1350      0.033     -4.092      0.000      -0.200      -0.070\n",
      "IDX_IMPORTANCE     0.0767      0.040      1.921      0.055      -0.002       0.155\n",
      "22-25             -0.0006      0.037     -0.017      0.986      -0.073       0.072\n",
      "18-21              0.0639      0.036      1.766      0.077      -0.007       0.135\n",
      "----------------------------------------------------------------------------------\n",
      "           y=1      dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "IDX_TRAD          -0.0177      0.045     -0.388      0.698      -0.107       0.071\n",
      "IDX_FIN           -0.0222      0.036     -0.625      0.532      -0.092       0.047\n",
      "IDX_IMPORTANCE    -0.1029      0.043     -2.414      0.016      -0.186      -0.019\n",
      "22-25              0.0008      0.039      0.022      0.983      -0.075       0.077\n",
      "18-21             -0.0929      0.039     -2.379      0.017      -0.170      -0.016\n",
      "----------------------------------------------------------------------------------\n",
      "           y=2      dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "IDX_TRAD          -0.1570      0.035     -4.455      0.000      -0.226      -0.088\n",
      "IDX_FIN            0.1572      0.028      5.627      0.000       0.102       0.212\n",
      "IDX_IMPORTANCE     0.0261      0.034      0.779      0.436      -0.040       0.092\n",
      "22-25             -0.0002      0.031     -0.007      0.995      -0.061       0.060\n",
      "18-21              0.0291      0.031      0.946      0.344      -0.031       0.089\n",
      "==================================================================================\n",
      "\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\mnlogit_coef.csv \n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\mnlogit_rrr.csv \n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\mnlogit_mfx_overall.csv\n"
     ]
    }
   ],
   "source": [
    "# 4D — MNLogit robusto (X float, y int) + RRR + Marginal Effects\n",
    "import numpy as np, pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\")\n",
    "IN   = BASE / \"_dataset_clean_enriched.csv\"\n",
    "df   = pd.read_csv(IN)\n",
    "print(\"Loaded enriched:\", IN, \"shape:\", df.shape)\n",
    "\n",
    "# 1) Dati e outcome numerico (0=Tradizionale, 1=Mix, 2=Fintech)\n",
    "data = df[df[\"future_pref_norm\"].isin([\"Tradizionale\",\"Mix\",\"Fintech\"])].copy()\n",
    "need = [\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\",\"age_band_norm\",\"future_pref_norm\"]\n",
    "data = data.dropna(subset=need).copy()\n",
    "\n",
    "cat = pd.Categorical(data[\"future_pref_norm\"], categories=[\"Tradizionale\",\"Mix\",\"Fintech\"])\n",
    "y = pd.Series(cat.codes, index=data.index, name=\"y\").astype(int)  # 0,1,2\n",
    "\n",
    "# 2) Costruisci X: indici + dummies fascia (baseline=26-30), + costante\n",
    "X = data[[\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\"]].copy()\n",
    "\n",
    "# Ordina le fasce e imposta baseline 26-30\n",
    "age = pd.Categorical(data[\"age_band_norm\"], categories=[\"26-30\",\"22-25\",\"18-21\"], ordered=True)\n",
    "age_dummies = pd.get_dummies(age, drop_first=True)  # colonne: '22-25','18-21' (baseline=26-30)\n",
    "X = pd.concat([X, age_dummies], axis=1)\n",
    "\n",
    "# Cast forzato a numerico (float); droppa righe con NaN post-cast\n",
    "for c in X.columns:\n",
    "    X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "mask = X.notna().all(axis=1) & y.notna()\n",
    "X = X.loc[mask].astype(float)\n",
    "y = y.loc[mask].astype(int)\n",
    "\n",
    "# Aggiungi costante\n",
    "X = sm.add_constant(X, has_constant=\"add\")\n",
    "\n",
    "print(\"\\nDtypes X (verifica):\")\n",
    "print(X.dtypes)\n",
    "\n",
    "# 3) Fit MNLogit\n",
    "mn = sm.MNLogit(y, X).fit(method=\"newton\", maxiter=200, disp=False)\n",
    "print(\"\\n[MNLogit] Preferenza futura ~ indici + fascia\")\n",
    "print(mn.summary())\n",
    "\n",
    "# 4) McFadden pseudo-R^2\n",
    "llf = mn.llf\n",
    "llnull = mn.llnull\n",
    "pseudo_r2 = 1 - (llf/llnull)\n",
    "print(f\"\\nMcFadden pseudo-R^2: {pseudo_r2:.3f}\")\n",
    "\n",
    "# 5) Relative Risk Ratios (exp(coef))\n",
    "rrr = np.exp(mn.params)\n",
    "print(\"\\nRelative Risk Ratios (exp(coef)):\")\n",
    "print(rrr.round(3))\n",
    "\n",
    "# 6) Marginal effects (AME)\n",
    "mfx = mn.get_margeff(at=\"overall\")\n",
    "print(\"\\nMarginal effects (overall):\")\n",
    "print(mfx.summary())\n",
    "\n",
    "# 7) Salva output “camera-ready”\n",
    "OUT_COEF = BASE / \"mnlogit_coef.csv\"\n",
    "OUT_RRR  = BASE / \"mnlogit_rrr.csv\"\n",
    "OUT_MFX  = BASE / \"mnlogit_mfx_overall.csv\"\n",
    "mn.params.round(4).to_csv(OUT_COEF, encoding=\"utf-8-sig\")\n",
    "rrr.round(4).to_csv(OUT_RRR, encoding=\"utf-8-sig\")\n",
    "import pandas as _pd\n",
    "_pd.DataFrame(mfx.summary().tables[1].data[1:], columns=mfx.summary().tables[1].data[0]).to_csv(OUT_MFX, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\nSaved:\", OUT_COEF, \"\\nSaved:\", OUT_RRR, \"\\nSaved:\", OUT_MFX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6f896",
   "metadata": {},
   "source": [
    "## **STEP 4E — VIF (diagnostica collinearità)**\n",
    "**Logica:** calcola **Variance Inflation Factor** sullo stesso design dello Step 4D\n",
    "(indici + dummies età), con cast duro a `float64`, drop NA e rimozione feature a varianza nulla.\n",
    "\n",
    "**Output:** `mnlogit_vif_fix.csv` con VIF ordinati decrescenti.\n",
    "\n",
    "**Next:** Step 4F passa a **RandomForest** e **HistGradientBoosting** per previsione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35d93ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF (design 4D, safe-cast float64):\n",
      "           feature       VIF\n",
      "0        IDX_TRAD  1.378867\n",
      "1         IDX_FIN  1.274683\n",
      "4           26-30  1.089411\n",
      "2  IDX_IMPORTANCE  1.057001\n",
      "3           22-25  1.025665\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\mnlogit_vif_fix.csv\n"
     ]
    }
   ],
   "source": [
    "# STEP 4E_fixVIF_safe — VIF coerente con 4D, con cast numerico duro (float64) e drop NA\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\")\n",
    "IN   = BASE / \"_dataset_clean_enriched.csv\"\n",
    "OUT  = BASE / \"mnlogit_vif_fix.csv\"\n",
    "\n",
    "df = pd.read_csv(IN)\n",
    "\n",
    "# Stesso design di 4D: indici + dummies fascia (baseline 26-30 => colonne '18-21' e '22-25')\n",
    "data = df[df[\"future_pref_norm\"].isin([\"Tradizionale\",\"Mix\",\"Fintech\"])].copy()\n",
    "need = [\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\",\"age_band_norm\",\"future_pref_norm\"]\n",
    "data = data.dropna(subset=need).copy()\n",
    "\n",
    "age_dum = pd.get_dummies(data[\"age_band_norm\"], drop_first=True)  # '18-21','22-25'\n",
    "X = pd.concat([data[[\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\"]], age_dum], axis=1)\n",
    "\n",
    "# --- (opzionale) standardizza SOLO gli indici, NON i dummies\n",
    "Xz = X.copy()\n",
    "for c in [\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\"]:\n",
    "    mu, sd = Xz[c].mean(), Xz[c].std(ddof=0)\n",
    "    if sd == 0 or np.isnan(sd):  # safety\n",
    "        Xz[c] = 0.0\n",
    "    else:\n",
    "        Xz[c] = (Xz[c] - mu) / sd\n",
    "\n",
    "# 1) Cast duro a float64 colonna-per-colonna\n",
    "for c in Xz.columns:\n",
    "    Xz[c] = pd.to_numeric(Xz[c], errors=\"coerce\")\n",
    "\n",
    "# 2) Drop righe con NaN (statsmodels non gestisce NA nel VIF)\n",
    "Xz = Xz.dropna(axis=0)\n",
    "\n",
    "# 3) Rimuovi eventuali colonne con varianza ~0 (singolarità nelle regressioni interne del VIF)\n",
    "keep_cols = []\n",
    "for c in Xz.columns:\n",
    "    if float(Xz[c].std(ddof=0)) > 0:\n",
    "        keep_cols.append(c)\n",
    "Xz = Xz[keep_cols]\n",
    "\n",
    "# 4) Ottenere una matrice numpy *pura* float64 (niente dtypes \"nullable\")\n",
    "A = Xz.to_numpy(dtype=np.float64)\n",
    "\n",
    "# 5) Calcola VIF (una colonna per volta)\n",
    "vifs = []\n",
    "for i in range(A.shape[1]):\n",
    "    vifs.append(variance_inflation_factor(A, i))\n",
    "\n",
    "vif_df = pd.DataFrame({\"feature\": Xz.columns, \"VIF\": vifs}).sort_values(\"VIF\", ascending=False)\n",
    "print(\"VIF (design 4D, safe-cast float64):\\n\", vif_df)\n",
    "\n",
    "vif_df.to_csv(OUT, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved:\", OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2445b8b",
   "metadata": {},
   "source": [
    "## **STEP 4F — Modelli predittivi (RF & HGB)**\n",
    "**Logica:** costruisce feature set numerico (Likert, indici, dummies età/prov, dummies use_*)\n",
    "e addestra **RandomForest** e **HistGradientBoosting** con CV su train.\n",
    "Valuta su test, salva **confusion matrix** e **importanze** (native e **permutation**).\n",
    "\n",
    "**Output:** immagini `cm_RF.png`, `cm_HGB.png`, `importances_*.{csv,png}`, `perm_importance_*.{csv,png}` in `ml_outputs/`.\n",
    "\n",
    "**Next:** Step 4G estrae **componenti ortogonali (PCA)** e rifà MNLogit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3344c61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded enriched: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\_dataset_clean_enriched.csv shape: (1064, 37)\n",
      "X shape: (1000, 29) | y distribution: [356 429 215] (0=Trad,1=Mix,2=Fintech)\n",
      "\n",
      "[RF] CV Accuracy: 0.473 ± 0.030 | CV F1-macro: 0.464 ± 0.037\n",
      "\n",
      "[HGB] CV Accuracy: 0.439 ± 0.022 | CV F1-macro: 0.432 ± 0.030\n",
      "\n",
      "[RF] TEST Accuracy=0.490 | F1-macro=0.478\n",
      "[RF] Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Trad       0.51      0.59      0.55        71\n",
      "         Mix       0.47      0.48      0.47        86\n",
      "     Fintech       0.52      0.35      0.42        43\n",
      "\n",
      "    accuracy                           0.49       200\n",
      "   macro avg       0.50      0.47      0.48       200\n",
      "weighted avg       0.49      0.49      0.49       200\n",
      "\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\ml_outputs\\cm_RF.png\n",
      "\n",
      "[HGB] TEST Accuracy=0.455 | F1-macro=0.436\n",
      "[HGB] Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Trad       0.47      0.59      0.52        71\n",
      "         Mix       0.46      0.42      0.44        86\n",
      "     Fintech       0.42      0.30      0.35        43\n",
      "\n",
      "    accuracy                           0.46       200\n",
      "   macro avg       0.45      0.44      0.44       200\n",
      "weighted avg       0.45      0.46      0.45       200\n",
      "\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\ml_outputs\\cm_HGB.png\n",
      "\n",
      "[RF] Top-20 feature_importances_:\n",
      " IDX_IMPORTANCE             0.0803\n",
      "IDX_TRAD                   0.0705\n",
      "imp_innovation             0.0599\n",
      "IDX_FIN                    0.0582\n",
      "imp_brand                  0.0510\n",
      "imp_branch                 0.0506\n",
      "imp_cashback               0.0470\n",
      "fin_trust                  0.0452\n",
      "prov_Banca tradizionale    0.0421\n",
      "imp_app                    0.0390\n",
      "trad_ux                    0.0378\n",
      "trad_innov                 0.0375\n",
      "imp_onboarding             0.0375\n",
      "prov_Neobanca/Fintech      0.0358\n",
      "trad_trust                 0.0347\n",
      "trad_service               0.0311\n",
      "trad_fees                  0.0291\n",
      "fin_fees                   0.0291\n",
      "fin_ux                     0.0278\n",
      "imp_no_fees                0.0240\n",
      "dtype: float64\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\ml_outputs\\importances_RF.csv\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\ml_outputs\\importances_RF.png\n",
      "\n",
      "[RF] Permutation importance (Top-20):\n",
      " trad_service               0.0437\n",
      "fin_trust                  0.0430\n",
      "imp_brand                  0.0370\n",
      "fin_fees                   0.0320\n",
      "imp_cashback               0.0315\n",
      "imp_branch                 0.0275\n",
      "prov_Neobanca/Fintech      0.0262\n",
      "IDX_IMPORTANCE             0.0250\n",
      "age_18-21                  0.0232\n",
      "IDX_FIN                    0.0230\n",
      "imp_app                    0.0217\n",
      "use_nessuno                0.0207\n",
      "trad_innov                 0.0193\n",
      "age_26-30                  0.0190\n",
      "prov_Banca tradizionale    0.0187\n",
      "IDX_TRAD                   0.0155\n",
      "age_22-25                  0.0153\n",
      "use_conti_pagamenti        0.0133\n",
      "use_credito_bnpl           0.0115\n",
      "imp_no_fees                0.0110\n",
      "dtype: float64\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\ml_outputs\\perm_importance_RF.csv\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\ml_outputs\\perm_importance_RF.png\n",
      "\n",
      "[HGB] Permutation importance (Top-20):\n",
      " trad_service               0.0363\n",
      "prov_Neobanca/Fintech      0.0305\n",
      "prov_Banca tradizionale    0.0275\n",
      "imp_branch                 0.0250\n",
      "imp_app                    0.0158\n",
      "trad_innov                 0.0155\n",
      "use_credito_bnpl           0.0148\n",
      "IDX_IMPORTANCE             0.0133\n",
      "fin_fees                   0.0113\n",
      "imp_no_fees                0.0110\n",
      "use_nessuno                0.0110\n",
      "age_26-30                  0.0090\n",
      "IDX_FIN                    0.0088\n",
      "imp_innovation             0.0085\n",
      "imp_brand                  0.0070\n",
      "IDX_TRAD                   0.0070\n",
      "imp_onboarding             0.0063\n",
      "imp_cashback               0.0058\n",
      "age_18-21                  0.0048\n",
      "fin_trust                  0.0035\n",
      "dtype: float64\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\ml_outputs\\perm_importance_HGB.csv\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\ml_outputs\\perm_importance_HGB.png\n",
      "\n",
      "[4F OK] Output salvati in: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\ml_outputs\n"
     ]
    }
   ],
   "source": [
    "# STEP 4F — Modelli predittivi (RandomForest & Gradient Boosting) su future_pref_norm\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\")\n",
    "IN   = BASE / \"_dataset_clean_enriched.csv\"\n",
    "OUTD = BASE / \"ml_outputs\"; OUTD.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df = pd.read_csv(IN)\n",
    "print(\"Loaded enriched:\", IN, \"shape:\", df.shape)\n",
    "\n",
    "# === 1) Target e feature set ===\n",
    "# Target: future_pref_norm (3 classi). Teniamo Trad/Mix/Fintech.\n",
    "df = df[df[\"future_pref_norm\"].isin([\"Tradizionale\",\"Mix\",\"Fintech\"])].copy()\n",
    "\n",
    "# Feature candidate numeriche\n",
    "likert_cols = [c for c in [\n",
    "    \"trad_trust\",\"trad_fees\",\"trad_innov\",\"trad_ux\",\"trad_service\",\n",
    "    \"fin_trust\",\"fin_fees\",\"fin_ux\",\"fin_service\",\n",
    "    \"imp_no_fees\",\"imp_brand\",\"imp_onboarding\",\"imp_branch\",\"imp_app\",\"imp_innovation\",\"imp_cashback\"\n",
    "] if c in df.columns]\n",
    "\n",
    "index_cols = [c for c in [\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\"] if c in df.columns]\n",
    "\n",
    "use_cols = [c for c in df.columns if c.startswith(\"use_\")]  # dummies tipi fintech (creati allo step 4A)\n",
    "\n",
    "# Dummies fascia età (baseline gestita dal modello; qui includiamo tutte)\n",
    "age_dum = pd.get_dummies(df[\"age_band_norm\"], prefix=\"age\", drop_first=False)\n",
    "\n",
    "# (Opzionale) Dummies provider principale attuale — se vuoi includerlo come segnale di preferenza\n",
    "prov_dum = pd.get_dummies(df.get(\"primary_provider_norm\", pd.Series(index=df.index)), prefix=\"prov\", drop_first=False)\n",
    "\n",
    "# Costruisci X finale (tutto numerico)\n",
    "X_list = []\n",
    "for cols in [likert_cols, index_cols, use_cols]:\n",
    "    if cols: X_list.append(df[cols])\n",
    "X_list += [age_dum, prov_dum]  # includi anche dummies\n",
    "X = pd.concat(X_list, axis=1)\n",
    "\n",
    "# Target codificato 0/1/2 nella solita ordine (Trad base interpretativa)\n",
    "y_cat = pd.Categorical(df[\"future_pref_norm\"], categories=[\"Tradizionale\",\"Mix\",\"Fintech\"])\n",
    "y = pd.Series(y_cat.codes, index=df.index).astype(int)\n",
    "\n",
    "print(f\"X shape: {X.shape} | y distribution: {np.bincount(y)} (0=Trad,1=Mix,2=Fintech)\")\n",
    "\n",
    "# === 2) Train/Test split stratificato ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# === 3) Pipeline con imputazione (median) ===\n",
    "rf = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=500, max_depth=None, min_samples_leaf=2,\n",
    "        random_state=42, n_jobs=-1, class_weight=None\n",
    "    ))\n",
    "])\n",
    "\n",
    "gb = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"clf\", HistGradientBoostingClassifier(\n",
    "        learning_rate=0.06, max_depth=None, max_iter=500,\n",
    "        min_samples_leaf=20, random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === 4) CV (5-fold stratificata) su training ===\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for name, model in [(\"RF\", rf), (\"HGB\", gb)]:\n",
    "    acc = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "    f1m = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"f1_macro\", n_jobs=-1)\n",
    "    print(f\"\\n[{name}] CV Accuracy: {acc.mean():.3f} ± {acc.std():.03f} | CV F1-macro: {f1m.mean():.3f} ± {f1m.std():.03f}\")\n",
    "\n",
    "# === 5) Fit su tutto il training e valutazione su test ===\n",
    "rf.fit(X_train, y_train)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "def eval_on_test(name, model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1m = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    print(f\"\\n[{name}] TEST Accuracy={acc:.3f} | F1-macro={f1m:.3f}\")\n",
    "    print(f\"[{name}] Classification report:\\n\", classification_report(\n",
    "        y_test, y_pred, target_names=[\"Trad\",\"Mix\",\"Fintech\"])\n",
    "    )\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # salva confusion matrix\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(f\"{name} — Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, [\"Trad\",\"Mix\",\"Fintech\"], rotation=45)\n",
    "    plt.yticks(tick_marks, [\"Trad\",\"Mix\",\"Fintech\"])\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "    plt.ylabel(\"True label\"); plt.xlabel(\"Predicted label\")\n",
    "    OUTP = OUTD / f\"cm_{name}.png\"\n",
    "    plt.tight_layout(); plt.savefig(OUTP, dpi=150); plt.close()\n",
    "    print(\"Saved:\", OUTP)\n",
    "    return y_pred\n",
    "\n",
    "y_pred_rf = eval_on_test(\"RF\", rf)\n",
    "y_pred_gb = eval_on_test(\"HGB\", gb)\n",
    "\n",
    "# === 6) Importanza delle feature ===\n",
    "# Per RF e HGB (tree-based) possiamo leggere feature_importances_\n",
    "def get_feature_names(Xdf): return list(Xdf.columns)\n",
    "\n",
    "feat_names = get_feature_names(X)\n",
    "\n",
    "def dump_importances(name, model):\n",
    "    # estrai il classificatore dalla pipeline\n",
    "    clf = model.named_steps[\"clf\"]\n",
    "    if hasattr(clf, \"feature_importances_\"):\n",
    "        imps = pd.Series(clf.feature_importances_, index=feat_names).sort_values(ascending=False)\n",
    "        top20 = imps.head(20)\n",
    "        print(f\"\\n[{name}] Top-20 feature_importances_:\\n\", top20.round(4))\n",
    "        imp_path = OUTD / f\"importances_{name}.csv\"\n",
    "        imps.to_csv(imp_path, header=[\"importance\"], encoding=\"utf-8-sig\")\n",
    "        print(\"Saved:\", imp_path)\n",
    "        # barplot top-20\n",
    "        plt.figure(figsize=(8,6))\n",
    "        top20[::-1].plot(kind=\"barh\")\n",
    "        plt.title(f\"{name} — Top-20 importances\")\n",
    "        plt.tight_layout()\n",
    "        OUTP = OUTD / f\"importances_{name}.png\"\n",
    "        plt.savefig(OUTP, dpi=150); plt.close()\n",
    "        print(\"Saved:\", OUTP)\n",
    "\n",
    "dump_importances(\"RF\", rf)\n",
    "dump_importances(\"HGB\", gb)\n",
    "\n",
    "# === 7) Permutation importance (più robusta) su test ===\n",
    "def perm_imp(name, model, n_repeats=10):\n",
    "    r = permutation_importance(\n",
    "        model, X_test, y_test, n_repeats=n_repeats, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    pi = pd.Series(r.importances_mean, index=feat_names).sort_values(ascending=False)\n",
    "    top20 = pi.head(20)\n",
    "    print(f\"\\n[{name}] Permutation importance (Top-20):\\n\", top20.round(4))\n",
    "    pi_path = OUTD / f\"perm_importance_{name}.csv\"\n",
    "    pi.to_csv(pi_path, header=[\"perm_imp_mean\"], encoding=\"utf-8-sig\")\n",
    "    print(\"Saved:\", pi_path)\n",
    "    # plot\n",
    "    plt.figure(figsize=(8,6))\n",
    "    top20[::-1].plot(kind=\"barh\")\n",
    "    plt.title(f\"{name} — Permutation importance (Top-20)\")\n",
    "    plt.tight_layout()\n",
    "    OUTP = OUTD / f\"perm_importance_{name}.png\"\n",
    "    plt.savefig(OUTP, dpi=150); plt.close()\n",
    "    print(\"Saved:\", OUTP)\n",
    "\n",
    "perm_imp(\"RF\", rf, n_repeats=20)\n",
    "perm_imp(\"HGB\", gb, n_repeats=20)\n",
    "\n",
    "print(\"\\n[4F OK] Output salvati in:\", OUTD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d277f1f",
   "metadata": {},
   "source": [
    "## **STEP 4G — PCA sugli item + MNLogit su PC**\n",
    "**Logica:** standardizza item Likert, estrae **3 componenti principali** (spesso >60% var. spiegata),\n",
    "salva **loadings** e **varianza spiegata**, e stima MNLogit su **PC + età**.\n",
    "\n",
    "**Output:** `pca_explained_variance.csv`, `pca_loadings.csv`, `mnlogit_pc_coef.csv`, `mnlogit_pc_rrr.csv`.\n",
    "\n",
    "**Next:** Step 4H applica **Logistic multinomiale regolarizzata** con CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06ce01d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: PC1    0.310\n",
      "PC2    0.105\n",
      "PC3    0.087\n",
      "dtype: float64\n",
      "Saved loadings & variance in: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\pca_outputs\n",
      "\n",
      "[MNLogit on PCs] summary\n",
      "                          MNLogit Regression Results                          \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                 1064\n",
      "Model:                        MNLogit   Df Residuals:                     1046\n",
      "Method:                           MLE   Df Model:                           15\n",
      "Date:                Wed, 10 Sep 2025   Pseudo R-squ.:                 0.07807\n",
      "Time:                        01:13:31   Log-Likelihood:                -1201.4\n",
      "converged:                       True   LL-Null:                       -1303.2\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.158e-35\n",
      "==============================================================================\n",
      "       y=0       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.6503      0.255      6.483      0.000       1.151       2.149\n",
      "PC1            0.2496      0.079      3.153      0.002       0.094       0.405\n",
      "PC2           -0.0318      0.115     -0.277      0.782      -0.257       0.193\n",
      "PC3            0.1411      0.122      1.159      0.246      -0.097       0.380\n",
      "22-25          0.1910      0.370      0.516      0.606      -0.535       0.917\n",
      "26-30         -0.4030      0.351     -1.149      0.251      -1.090       0.284\n",
      "------------------------------------------------------------------------------\n",
      "       y=1       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.8686      0.250      7.481      0.000       1.379       2.358\n",
      "PC1           -0.0780      0.072     -1.080      0.280      -0.220       0.064\n",
      "PC2           -0.0296      0.113     -0.262      0.793      -0.251       0.192\n",
      "PC3           -0.1221      0.119     -1.023      0.306      -0.356       0.112\n",
      "22-25          0.2522      0.365      0.691      0.490      -0.463       0.968\n",
      "26-30         -0.0749      0.346     -0.216      0.829      -0.753       0.604\n",
      "------------------------------------------------------------------------------\n",
      "       y=2       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.9592      0.273      3.517      0.000       0.425       1.494\n",
      "PC1           -0.3918      0.076     -5.125      0.000      -0.542      -0.242\n",
      "PC2           -0.0188      0.123     -0.153      0.879      -0.260       0.223\n",
      "PC3            0.0622      0.130      0.479      0.632      -0.192       0.317\n",
      "22-25          0.0986      0.388      0.254      0.799      -0.661       0.859\n",
      "26-30          0.0207      0.380      0.054      0.957      -0.724       0.765\n",
      "==============================================================================\n",
      "McFadden pseudo-R^2 (PC): 0.078\n",
      "Saved MNLogit PC coef/RRR in: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\pca_outputs\n"
     ]
    }
   ],
   "source": [
    "# STEP 4G — PCA sugli item (o sugli indici) + MNLogit su componenti ortogonali\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\")\n",
    "IN   = BASE / \"_dataset_clean_enriched.csv\"\n",
    "OUTD = BASE / \"pca_outputs\"; OUTD.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df = pd.read_csv(IN)\n",
    "\n",
    "# 1) Scegli feature per PCA: qui uso TUTTI gli item Likert (più informativi)\n",
    "likert_cols = [c for c in [\n",
    "    \"trad_trust\",\"trad_fees\",\"trad_innov\",\"trad_ux\",\"trad_service\",\n",
    "    \"fin_trust\",\"fin_fees\",\"fin_ux\",\"fin_service\",\n",
    "    \"imp_no_fees\",\"imp_brand\",\"imp_onboarding\",\"imp_branch\",\"imp_app\",\"imp_innovation\",\"imp_cashback\"\n",
    "] if c in df.columns]\n",
    "\n",
    "d = df.dropna(subset=likert_cols + [\"age_band_norm\",\"future_pref_norm\"]).copy()\n",
    "\n",
    "X_items = d[likert_cols].values\n",
    "X_std   = StandardScaler().fit_transform(X_items)\n",
    "\n",
    "# 2) PCA: tieni prime 3 componenti (spesso catturano >60% var.)\n",
    "pca = PCA(n_components=3, random_state=42)\n",
    "PC  = pca.fit_transform(X_std)\n",
    "\n",
    "expl_var = pd.Series(pca.explained_variance_ratio_, index=[f\"PC{i+1}\" for i in range(PC.shape[1])])\n",
    "print(\"Explained variance ratio:\", expl_var.round(3))\n",
    "expl_var.to_csv(OUTD / \"pca_explained_variance.csv\", header=[\"explained_var_ratio\"], encoding=\"utf-8-sig\")\n",
    "\n",
    "# Salva loadings per interpretazione\n",
    "loadings = pd.DataFrame(pca.components_.T, index=likert_cols, columns=[f\"PC{i+1}\" for i in range(PC.shape[1])])\n",
    "loadings.to_csv(OUTD / \"pca_loadings.csv\", encoding=\"utf-8-sig\")\n",
    "print(\"Saved loadings & variance in:\", OUTD)\n",
    "\n",
    "# 3) MNLogit su PC + fascia\n",
    "keep_idx = d.index\n",
    "pc_df = pd.DataFrame(PC, index=keep_idx, columns=[f\"PC{i+1}\" for i in range(PC.shape[1])])\n",
    "\n",
    "y_cat = pd.Categorical(d.loc[keep_idx, \"future_pref_norm\"], categories=[\"Tradizionale\",\"Mix\",\"Fintech\"])\n",
    "y = pd.Series(y_cat.codes, index=keep_idx).astype(int)\n",
    "\n",
    "age_dum = pd.get_dummies(d.loc[keep_idx, \"age_band_norm\"], drop_first=True)  # baseline 26-30\n",
    "Xp = pd.concat([pc_df, age_dum], axis=1).astype(float)\n",
    "Xp = sm.add_constant(Xp)\n",
    "\n",
    "mn_pc = sm.MNLogit(y, Xp).fit(method=\"newton\", maxiter=200, disp=False)\n",
    "print(\"\\n[MNLogit on PCs] summary\")\n",
    "print(mn_pc.summary())\n",
    "\n",
    "# Confronto pseudo-R2 con 4D (se vuoi)\n",
    "pseudo_r2_pc = 1 - (mn_pc.llf / mn_pc.llnull)\n",
    "print(f\"McFadden pseudo-R^2 (PC): {pseudo_r2_pc:.3f}\")\n",
    "\n",
    "mn_pc.params.round(4).to_csv(OUTD / \"mnlogit_pc_coef.csv\", encoding=\"utf-8-sig\")\n",
    "np.exp(mn_pc.params).round(4).to_csv(OUTD / \"mnlogit_pc_rrr.csv\", encoding=\"utf-8-sig\")\n",
    "print(\"Saved MNLogit PC coef/RRR in:\", OUTD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b75f42",
   "metadata": {},
   "source": [
    "## **STEP 4H — Logistic multinomiale L2 con CV**\n",
    "**Logica:** pipeline con **StandardScaler** (solo indici) + **LogisticRegressionCV (multinomial)**\n",
    "su indici + dummies età + dummies provider. Salva **coef** completi, **contrasti** vs baseline\n",
    "e **RRR** corrispondenti.\n",
    "\n",
    "**Output:** `logreg_cv_coef_full.csv`, `logreg_cv_coef_contrasts_vs_trad.csv`, `logreg_cv_rrr_vs_trad.csv`,\n",
    "`logreg_cv_C_selected.csv`, `logreg_cv_confusion_matrix.csv`.\n",
    "\n",
    "**Next:** Step 4I calibra le probabilità via **isotonic** e valuta il **Brier score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d4c4a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LR-CV] TEST Acc=0.468 | F1-macro=0.433\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Trad       0.47      0.85      0.61        89\n",
      "         Mix       0.45      0.17      0.24       107\n",
      "     Fintech       0.47      0.43      0.45        54\n",
      "\n",
      "    accuracy                           0.47       250\n",
      "   macro avg       0.46      0.48      0.43       250\n",
      "weighted avg       0.46      0.47      0.42       250\n",
      "\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\ml_outputs\\logreg_cv_coef_full.csv\n",
      "ATTENZIONE: baseline 'Tradizionale' non trovata in clf.classes_. Coefficienti completi salvati, contrasti saltati.\n",
      "Saved: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\ml_outputs\\logreg_cv_C_selected.csv\n"
     ]
    }
   ],
   "source": [
    "# STEP 4H_fix — Multinomial Logistic REGOLARIZZATA (L2) con CV, feature names, contrasti vs baseline e RRR\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\")\n",
    "IN   = BASE / \"_dataset_clean_enriched.csv\"\n",
    "OUTD = BASE / \"ml_outputs\"; OUTD.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# --- 1) Dati e feature set\n",
    "df = pd.read_csv(IN)\n",
    "df = df[df[\"future_pref_norm\"].isin([\"Tradizionale\",\"Mix\",\"Fintech\"])].copy()\n",
    "\n",
    "# costruiamo X: indici + dummies fascia + dummies provider (se presente)\n",
    "num_cols   = [\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\"]\n",
    "age_dum    = pd.get_dummies(df[\"age_band_norm\"], drop_first=True)   # baseline=26-30\n",
    "prov_dum   = pd.get_dummies(df.get(\"primary_provider_norm\", pd.Series(index=df.index)), drop_first=True)\n",
    "\n",
    "X = pd.concat([df[num_cols], age_dum, prov_dum], axis=1)\n",
    "# y con ordine fissato\n",
    "class_order = [\"Tradizionale\",\"Mix\",\"Fintech\"]\n",
    "y = pd.Categorical(df[\"future_pref_norm\"], categories=class_order, ordered=True).codes\n",
    "\n",
    "# --- 2) Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# --- 3) Pipeline: standardizza SOLO gli indici (non i dummies)\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[(\"scale_idx\", StandardScaler(with_mean=True, with_std=True), num_cols)],\n",
    "    remainder=\"passthrough\"  # lascia invariati i dummies\n",
    ")\n",
    "\n",
    "logitcv = LogisticRegressionCV(\n",
    "    Cs=20,\n",
    "    cv=5,\n",
    "    penalty=\"l2\",\n",
    "    solver=\"lbfgs\",          # multinomial di default da sklearn 1.5+\n",
    "    scoring=\"f1_macro\",\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"ct\", ct), (\"clf\", logitcv)])\n",
    "\n",
    "# --- 4) Fit + metriche test\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1m = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(f\"[LR-CV] TEST Acc={acc:.3f} | F1-macro={f1m:.3f}\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Trad\",\"Mix\",\"Fintech\"]))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "pd.DataFrame(cm, index=class_order, columns=class_order).to_csv(OUTD / \"logreg_cv_confusion_matrix.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# --- 5) Coefficienti\n",
    "clf = pipe.named_steps[\"clf\"]\n",
    "\n",
    "# nomi feature post-CT (scaler + passthrough)\n",
    "feat_names = pipe.named_steps[\"ct\"].get_feature_names_out(X.columns)  # sklearn >=1.0\n",
    "feat_names = [f.replace(\"scale_idx__\", \"\") for f in feat_names]       # pulizia prefisso\n",
    "\n",
    "# coef_ ha una riga per classe nell'ordine clf.classes_\n",
    "coef_full = pd.DataFrame(clf.coef_, columns=feat_names, index=[f\"{c}\" for c in clf.classes_])\n",
    "coef_full.to_csv(OUTD / \"logreg_cv_coef_full.csv\", encoding=\"utf-8-sig\")\n",
    "print(\"Saved:\", OUTD / \"logreg_cv_coef_full.csv\")\n",
    "\n",
    "# Contrasti vs baseline 'Tradizionale': sottrai la riga della baseline alle altre (log-odds difference)\n",
    "baseline = \"Tradizionale\"\n",
    "if baseline in coef_full.index:\n",
    "    contr = coef_full.loc[[c for c in coef_full.index if c != baseline]].copy()\n",
    "    contr = contr.subtract(coef_full.loc[baseline].values, axis=1)\n",
    "    # rinomina righe in \"Classe_vs_Tradizionale\"\n",
    "    contr.index = [f\"{c}_vs_{baseline}\" for c in contr.index]\n",
    "    contr.to_csv(OUTD / \"logreg_cv_coef_contrasts_vs_trad.csv\", encoding=\"utf-8-sig\")\n",
    "    # Relative Risk Ratios (exp)\n",
    "    rrr = np.exp(contr)\n",
    "    rrr.to_csv(OUTD / \"logreg_cv_rrr_vs_trad.csv\", encoding=\"utf-8-sig\")\n",
    "    print(\"Saved:\", OUTD / \"logreg_cv_coef_contrasts_vs_trad.csv\")\n",
    "    print(\"Saved:\", OUTD / \"logreg_cv_rrr_vs_trad.csv\")\n",
    "else:\n",
    "    print(\"ATTENZIONE: baseline 'Tradizionale' non trovata in clf.classes_. Coefficienti completi salvati, contrasti saltati.\")\n",
    "\n",
    "# --- 6) Salva anche le C selezionate\n",
    "C_sel = getattr(clf, \"C_\", None)\n",
    "pd.DataFrame({\"C_selected\": np.atleast_1d(C_sel)}).to_csv(OUTD / \"logreg_cv_C_selected.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved:\", OUTD / \"logreg_cv_C_selected.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20208608",
   "metadata": {},
   "source": [
    "## **STEP 4I — Calibrazione probabilità + Brier score**\n",
    "**Logica:** calibra un **RandomForest** (isotonic CV=5) e valuta la bontà delle probabilità con\n",
    "**Brier score** per classe (One-Vs-Rest) e media.\n",
    "\n",
    "**Output:** `calibrated_probs_test.csv` con colonne `p_Trad, p_Mix, p_Fintech` e stampa Brier.\n",
    "\n",
    "**Next:** Step 4J costruisce i **PDP** per interpretare HGB sulle feature più importanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5b45739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brier (per classe 0=Trad,1=Mix,2=Fintech): [0.2048, 0.2563, 0.157] | mean: 0.206\n",
      "Saved calibrated probs: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\ml_outputs\\calibrated_probs_test.csv\n"
     ]
    }
   ],
   "source": [
    "# STEP 4I — Calibration + Brier score (probabilità ben tarate per use-case manageriali)\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\")\n",
    "IN   = BASE / \"_dataset_clean_enriched.csv\"\n",
    "OUTD = BASE / \"ml_outputs\"; OUTD.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df = pd.read_csv(IN)\n",
    "df = df[df[\"future_pref_norm\"].isin([\"Tradizionale\",\"Mix\",\"Fintech\"])].copy()\n",
    "\n",
    "likert_cols = [c for c in [\n",
    "    \"trad_trust\",\"trad_fees\",\"trad_innov\",\"trad_ux\",\"trad_service\",\n",
    "    \"fin_trust\",\"fin_fees\",\"fin_ux\",\"fin_service\",\n",
    "    \"imp_no_fees\",\"imp_brand\",\"imp_onboarding\",\"imp_branch\",\"imp_app\",\"imp_innovation\",\"imp_cashback\"\n",
    "] if c in df.columns]\n",
    "use_cols = [c for c in df.columns if c.startswith(\"use_\")]\n",
    "age_dum  = pd.get_dummies(df[\"age_band_norm\"], drop_first=False)\n",
    "prov_dum = pd.get_dummies(df.get(\"primary_provider_norm\", pd.Series(index=df.index)), drop_first=False)\n",
    "\n",
    "X = pd.concat([df[likert_cols], age_dum, prov_dum, df[[\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\"]], df[use_cols]], axis=1)\n",
    "y = pd.Categorical(df[\"future_pref_norm\"], categories=[\"Tradizionale\",\"Mix\",\"Fintech\"]).codes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n",
    "\n",
    "base = RandomForestClassifier(n_estimators=600, min_samples_leaf=2, random_state=42, n_jobs=-1)\n",
    "cal  = CalibratedClassifierCV(base, method=\"isotonic\", cv=5)\n",
    "cal.fit(X_train, y_train)\n",
    "\n",
    "probs = cal.predict_proba(X_test)\n",
    "# Brier score macro (media sulle classi in OVR)\n",
    "briers = []\n",
    "for k in range(3):\n",
    "    y_bin = (y_test == k).astype(int)\n",
    "    briers.append(brier_score_loss(y_bin, probs[:,k]))\n",
    "print(\"Brier (per classe 0=Trad,1=Mix,2=Fintech):\", [round(b,4) for b in briers], \"| mean:\", round(float(np.mean(briers)),4))\n",
    "\n",
    "pd.DataFrame(probs, columns=[\"p_Trad\",\"p_Mix\",\"p_Fintech\"]).to_csv(OUTD / \"calibrated_probs_test.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved calibrated probs:\", OUTD / \"calibrated_probs_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b93216d",
   "metadata": {},
   "source": [
    "## **STEP 4J — Partial Dependence (PDP) su HGB**\n",
    "**Logica:** per evitare `feature_importances_` (non sempre esposta da HGB), ordina le feature via\n",
    "**Permutation Importance** sul train, seleziona le **Top-8** e disegna i **PDP** per la classe *Fintech* (2).\n",
    "Si usano **indici di colonna** per robustezza verso pipeline/feature names.\n",
    "\n",
    "**Output:** `pdp_perm_importance_HGB_train.csv` e figura `pdp_hgb_top8.png` in `ml_outputs/`.\n",
    "\n",
    "**Next:** fine del capitolo; questi PDP forniscono insight locali sulle feature più rilevanti per HGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfb157a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-8 per PDP: ['imp_brand', 'Neobanca/Fintech', 'IDX_TRAD', 'IDX_IMPORTANCE', 'trad_service', 'Banca tradizionale', 'imp_innovation', 'imp_onboarding']\n",
      "Saved PDP: C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\\ml_outputs\\pdp_hgb_top8.png\n"
     ]
    }
   ],
   "source": [
    "# STEP 4J — Partial Dependence (PDP) per HGB sulle 8 feature più importanti (interpretabilità)\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.inspection import PartialDependenceDisplay, permutation_importance\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\Jacopo\\Tesi_Fintech_DEF\\dataset_pulito\")\n",
    "IN   = BASE / \"_dataset_clean_enriched.csv\"\n",
    "OUTD = BASE / \"ml_outputs\"; OUTD.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df = pd.read_csv(IN)\n",
    "df = df[df[\"future_pref_norm\"].isin([\"Tradizionale\",\"Mix\",\"Fintech\"])].copy()\n",
    "\n",
    "likert_cols = [c for c in [\n",
    "    \"trad_trust\",\"trad_fees\",\"trad_innov\",\"trad_ux\",\"trad_service\",\n",
    "    \"fin_trust\",\"fin_fees\",\"fin_ux\",\"fin_service\",\n",
    "    \"imp_no_fees\",\"imp_brand\",\"imp_onboarding\",\"imp_branch\",\"imp_app\",\"imp_innovation\",\"imp_cashback\"\n",
    "] if c in df.columns]\n",
    "use_cols = [c for c in df.columns if c.startswith(\"use_\")]\n",
    "age_dum  = pd.get_dummies(df[\"age_band_norm\"], drop_first=False)\n",
    "prov_dum = pd.get_dummies(df.get(\"primary_provider_norm\", pd.Series(index=df.index)), drop_first=False)\n",
    "\n",
    "X = pd.concat([df[likert_cols], age_dum, prov_dum, df[[\"IDX_TRAD\",\"IDX_FIN\",\"IDX_IMPORTANCE\"]], df[use_cols]], axis=1)\n",
    "y = pd.Categorical(df[\"future_pref_norm\"], categories=[\"Tradizionale\",\"Mix\",\"Fintech\"]).codes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.06, max_iter=500, min_samples_leaf=20, random_state=42\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "# === Rank delle feature con permutation importance (compatibile con HGB)\n",
    "# Filtra eventuali colonne a varianza zero nel training (non informative)\n",
    "non_const_cols = X_train.columns[X_train.var() > 0]\n",
    "X_train_nc = X_train[non_const_cols]\n",
    "\n",
    "perm = permutation_importance(hgb, X_train_nc, y_train, n_repeats=20, random_state=42, n_jobs=-1)\n",
    "pi = pd.Series(perm.importances_mean, index=non_const_cols).sort_values(ascending=False)\n",
    "\n",
    "# salva ranking completo\n",
    "pi.to_csv(OUTD / \"pdp_perm_importance_HGB_train.csv\", header=[\"perm_imp_mean\"], encoding=\"utf-8-sig\")\n",
    "\n",
    "top8 = list(pi.head(8).index)\n",
    "print(\"Top-8 per PDP:\", top8)\n",
    "\n",
    "# === PDP sulla classe \"Fintech\" (2) — usa indici di colonna per compatibilità\n",
    "feature_indices = [list(X_train.columns).index(c) for c in top8]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "PartialDependenceDisplay.from_estimator(hgb, X_train, features=feature_indices, target=2, ax=ax)\n",
    "plt.tight_layout()\n",
    "OUTP = OUTD / \"pdp_hgb_top8.png\"\n",
    "plt.savefig(OUTP, dpi=150); plt.close()\n",
    "print(\"Saved PDP:\", OUTP)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
